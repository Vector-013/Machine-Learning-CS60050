{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emjAQ_34SozP",
        "tags": []
      },
      "source": [
        "# Programming Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKkKCHwsSozQ"
      },
      "source": [
        "In this programming assignment, you will implement a linear regression model and a logistic regression model.\n",
        "\n",
        "In Part 1, you have to implement a linear regression model to predict the price of a house based on various input features.\n",
        "\n",
        "In Part 2, you have to implement a logistic regression model to predict the species of a grain using various morphological features.\n",
        "\n",
        "The assignment zip file (ML_Assignment_1.zip) contains 4 datasets which will be used in this assignment.\n",
        "\n",
        "You have to write your code in this jupyter notebook and submit the solved jupyter notebook with the file name \\<Roll_No\\>_A1.ipynb for evaluation. You have to enter your code only in those cells which are marked as ```## CODE REQUIRED ##```, and you have to write your code only between ```### START CODE HERE ###``` and ```### END CODE HERE ###``` comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHPf8pHpSozR"
      },
      "source": [
        "## Part 1: Linear Regression\n",
        "\n",
        "### Problem Statement  \n",
        "A real estate company is building a machine learning model to determine the price of a house. The model will take various information regarding a house as input features and predict the price per unit area. They decided to use the linear regression as the machine learning model. Your task is to help the company to build the model.\n",
        "Given various features of a house, you will create a linear regression model to predict the price of the house.\n",
        "\n",
        "### Data Description\n",
        "\n",
        "**For Even Roll Number Students:**\n",
        "\n",
        "Dataset Filename: Taiwan_House.csv\n",
        "\n",
        "Attributes Information:\n",
        "1. Transaction date\n",
        "2. House age\n",
        "3. Distance to the nearest MRT station\n",
        "4. Number of convenience stores\n",
        "5. Latitude\n",
        "6. Longitude\n",
        "\n",
        "Target variable: house price of unit area\n",
        "\n",
        "**For Odd Roll Number Students:**\n",
        "\n",
        "Dataset Filename: Boston_House.csv\n",
        "\n",
        "Attributes Information:\n",
        "1. CRIM: per capita crime rate by town\n",
        "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "3. INDUS: proportion of non-­retail business acres per town\n",
        "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "5. NOX: nitric oxides concentration (parts per 10 million)\n",
        "6. RM: average number of rooms per dwelling\n",
        "7. AGE: proportion of owner­occupied units built prior to 1940\n",
        "8. DIS: weighted distances to five Boston employment centres\n",
        "9. RAD: index of accessibility to radial highways\n",
        "10. TAX: full-­value property-­tax rate per $10,000\n",
        "\n",
        "11. PTRATIO: pupil-­teacher ratio by town\n",
        "12. B: 1000(Bk ­- 0.63)^2 where Bk is the proportion of blacks by town\n",
        "13. LSTAT: % lower status of the population\n",
        "\n",
        "Target Variable: MEDV: Median value of owner-­occupied homes in $1000's\n",
        "\n",
        "\n",
        "These are the following steps or functions that you have to complete to create and train the linear regression model:\n",
        "1. Reading the data\n",
        "2. Computing the loss function\n",
        "3. Computing the gradient of the loss\n",
        "4. Training the model using Batch Gradient Descent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rJDNL8y8SozR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5ExYIXiSozS"
      },
      "source": [
        "### 1.1. Reading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgnxuEDkSozS"
      },
      "source": [
        "In the following function ```load_data```, you have to read the data from the file and store the data into a pandas dataframe. Then you have to create two numpy arrays $X$ and $y$ from the dataframe:\n",
        "\n",
        "+ $X$: Input data of the shape (number of samples, number of input features)\n",
        "+ $y$: Target variable of the shape (number of samples,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DEal5SFmSozS",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X:  (506, 13) Shape of y:  (506,)\n"
          ]
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def load_data(filepath):\n",
        "    \"\"\"\n",
        "    This function loads the data into a pandas dataframe and coverts it into X and y numpy arrays\n",
        "\n",
        "    Args:\n",
        "        filepath: File path as a string\n",
        "    Returns:\n",
        "        X: Input data of the shape (# of samples, # of input features)\n",
        "        y: Target variable of the shape (# of sample,)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    data = pd.read_csv(filepath)\n",
        "    \n",
        "    target_coloumn = data.columns[-1]\n",
        "    X = data.drop(columns=[target_coloumn]).values\n",
        "    y = data[target_coloumn].values\n",
        "    \n",
        "    # print(type(X)), print(type(y))\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X,y\n",
        "\n",
        "filepath = None\n",
        "### START CODE HERE ###\n",
        "## set the file path\n",
        "filepath = './Boston_House.csv'\n",
        "### END CODE HERE ###\n",
        "X, y = load_data(filepath)\n",
        "\n",
        "print(\"Shape of X: \",X.shape, \"Shape of y: \",y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se3Qrch5PoIO"
      },
      "source": [
        "We will not use all the features from ```X```.\n",
        "\n",
        "**For Even Roll Number Students:**\n",
        "Set the last two digits of your roll number as the random seed and pick a number ``r`` between 3 and 6 (both inclusive) randomly. Use the first ```r``` features of the numpy array ```X```.\n",
        "\n",
        "**For Odd Roll Number Students:**\n",
        "Set the last two digits of your roll number as the random seed and pick a number ``r`` between 9 and 13 (both inclusive) randomly. Use the first ```r``` features of the numpy array ```X```.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mnRd0p8LtRJ5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "Shape of X:  (506, 10) Shape of y:  (506,)\n"
          ]
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def random_feature_selection(X):\n",
        "    \"\"\"\n",
        "    For Even Roll Number Students: Set the last two digits of your roll number as the random seed and pick a number r\n",
        "    between 3 and 6 randomly. Use the first ```r``` features of the numpy array X.\n",
        "    For Odd Roll Number Students: Set the last two digits of your roll number as the random seed and pick a number r\n",
        "    between 9 and 13 randomly. Use the first ```r``` features of the numpy array X.\n",
        "    Args:\n",
        "        X: Input data of the shape (# of samples, # of input features)\n",
        "    Returns:\n",
        "        X_new: New input data of the shape (# of samples, r) containg only the first r features from X\n",
        "    \"\"\"\n",
        "\n",
        "    X_new = None\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    random.seed(39)\n",
        "    r = random.randint(9,13)\n",
        "    print(r)\n",
        "    X_new = X[:, :r]\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X_new\n",
        "\n",
        "X = random_feature_selection(X)\n",
        "print(\"Shape of X: \",X.shape, \"Shape of y: \",y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FAc8UixtqLS"
      },
      "source": [
        "We need to pre-process the data. We are using min-max scaler to scale the input data ($X$).\n",
        "\n",
        "After that, we split the data (```X``` and ```y```) into a training dataset (```X_train``` and ```y_train```) and test dataset (```X_test``` and ```y_test```)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LLmD1I3-SozT",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train:  (379, 10) Shape of y_train:  (379,)\n",
            "Shape of X_test:  (127, 10) Shape of y_test:  (127,)\n"
          ]
        }
      ],
      "source": [
        "## Data scaling and train-test split\n",
        "\n",
        "def train_test_split(X, y, test_size=0.25, random_state=None):\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    split_index = int(X.shape[0] * (1 - test_size))\n",
        "\n",
        "    train_indices = indices[:split_index]\n",
        "    test_indices = indices[split_index:]\n",
        "\n",
        "    X_train = X[train_indices]\n",
        "    X_test = X[test_indices]\n",
        "    y_train = y[train_indices]\n",
        "    y_test = y[test_indices]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def min_max_scaler(X, feature_range=(0, 1)):\n",
        "    X_min = np.min(X, axis=0)\n",
        "    X_max = np.max(X, axis=0)\n",
        "\n",
        "    X_scaled = (X-X_min)/(X_max-X_min)\n",
        "\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "X = min_max_scaler(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "print(\"Shape of X_train: \",X_train.shape, \"Shape of y_train: \",y_train.shape)\n",
        "print(\"Shape of X_test: \",X_test.shape, \"Shape of y_test: \",y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrFEUV2-SozU"
      },
      "source": [
        "### 1.2. Computing the Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1BC0TPCSozU"
      },
      "source": [
        "In linear regression, the model parameters are:\n",
        "\n",
        "+ $w$: Parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "\n",
        "+ $b$: Bias parameter (scalar) of the linear regression model\n",
        "\n",
        "Both $w$ and $b$ are numpy arrays.\n",
        "\n",
        "Given the model parameters $w$ and $b$, the prediction for an input sample $X^i$ is:\n",
        "$$h_{w,b}(X^i) = w \\cdot X^i + b$$\n",
        "where $X^i$ is the $i^{th}$ training sample with shape (number of features,1)\n",
        "\n",
        "For linear regression, you have to implement and compute Mean Squared Error loss fucntion:\n",
        "$$ L_{w,b}(X) = \\sum_{i=1}^{m}(y^i - h_{w,b}(X^i))^2 $$\n",
        "where $y^i$ is the true target value for the $i^{th}$ sample and $h_{w,b}(X^i)$ is the predicted value for the $i^{th}$ sample using the parameters $w$ and $b$.\n",
        "\n",
        "$w$ is the list of parameters excluding the bias and $b$ is the bias term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t6GqZMlYSozV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def loss_function(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost function for linear regression.\n",
        "\n",
        "    Args:\n",
        "        X: Input data of the shape (# of training samples, # of input features)\n",
        "        y: Target variable of the shape (# of training sample,)\n",
        "        w: Parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "        b: Bias parameter (scalar) of the linear regression model\n",
        "\n",
        "    Returns\n",
        "        loss: The loss function value of using w and b as the parameters to fit the data points in X and y\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m = X.shape[0]\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # print(X.shape, w.shape)\n",
        "    \n",
        "    loss = np.sum((y - (np.dot(X, w.T) + b).flatten())**2)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0h7jhxGSozV"
      },
      "source": [
        "### 1.3. Comptuing the Gradient of the Loss\n",
        "\n",
        "In this following function ```compute_gradient```, you have to compute the gradients $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ of the loss $L$ w.r.t. $w$ and $b$. More specifically, you have to iterate over every training example and compute the gradients of the loss for that training example. Finally, aggregate the gradient values for all the training examples and take the average. The gradients can be computed as:\n",
        "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m (h_{w,b}(X^i)-y^i)X^i$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{w,b}(X^i)-y^i)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "do-IiCTZSozV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def compute_gradient(X, y, w, b):\n",
        "   \"\"\"\n",
        "   Computes the gradient values\n",
        "   Args:\n",
        "      X: Input data of the shape (# of training samples, # of input features)\n",
        "      y: Target variable of the shape (# of training sample,)\n",
        "      w: Parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "      b: Bias parameter of the linear regression model of the shape (1,1) or a scaler\n",
        "   Returns:\n",
        "      dL_dw : The gradient of the cost w.r.t. the parameters w with shape same as w\n",
        "      dL_db : The gradient of the cost w.r.t. the parameter b with shape same as b\n",
        "   \"\"\"\n",
        "\n",
        "   # Number of training examples\n",
        "   m = X.shape[0]\n",
        "\n",
        "   dL_dw = None\n",
        "   dL_db = None\n",
        "\n",
        "   ### START CODE HERE ###\n",
        "   dL_dw = np.dot((-y + (np.dot(X, w.T) + b).flatten()).T, X)/m\n",
        "   dL_db = np.sum(-y + (np.dot(X, w.T) + b).flatten())/m\n",
        "   \n",
        "\n",
        "   ### END CODE HERE ###\n",
        "   return dL_dw, dL_db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYL1jWaZSozV"
      },
      "source": [
        "### 1.4. Training the Model using Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vwyRzWqSozV"
      },
      "source": [
        "Finally, you have to implement the batch gradient descent algorithm to train and learn the parameters of the linear regression model. You have to use ```loss_function``` and ```compute_gradient``` functions that you have implemented earlier in this assignment.\n",
        "\n",
        "In this ```batch_gradient_descent``` function, you have to compute the gradient for the training samples and update the parameters $w$ and $b$ in every iteration:\n",
        "\n",
        "+ $w \\leftarrow w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
        "\n",
        "+ $b \\leftarrow b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
        "\n",
        "Additionally, you have compute the loss function values in every iteration and store it in the list variable ```loss_hist``` and print the loss value after every 100 iterations during the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UHKldb3hSozV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def batch_gradient_descent(X, y, w_initial, b_initial, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Batch gradient descent to learn the parameters (w and b) of the linear regression model and to print loss values\n",
        "    every 100 iterations\n",
        "\n",
        "    Args:\n",
        "        X: Input data of the shape (# of training samples, # of input features)\n",
        "        y: Target variable of the shape (# of training sample,)\n",
        "        w_initial: Initial parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "        b_initial: Initial bias parameter (scalar) of the linear regression model\n",
        "        alpha: Learning rate\n",
        "        num_iters: number of iterations\n",
        "    Returns\n",
        "        w: Updated values of parameters of the model after training\n",
        "        b: Updated bias of the model after training\n",
        "        loss_hist: List of loss values for every iteration\n",
        "    \"\"\"\n",
        "\n",
        "    # number of training examples\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # to store loss values for every iteation as a list and print loss value after every 100 iterations\n",
        "    loss_hist = []\n",
        "\n",
        "    # Initialize parameters\n",
        "    w = copy.deepcopy(w_initial) ## deepcopy is used so that the updates do not change the initial variable values\n",
        "    b = b_initial\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        dL_dw, dL_db = compute_gradient(X, y, w, b)\n",
        "        w = w - alpha*dL_dw\n",
        "        b = b - alpha*dL_db\n",
        "        loss = loss_function(X, y, w, b)\n",
        "        loss_hist.append(loss)\n",
        "        if i%100 == 0:\n",
        "            print(f\"Loss after iteration {i} is {loss}\")\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return w, b, loss_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0h3DUMwNocp"
      },
      "source": [
        "Now you have to intialize the model parameters ($w$ and $b$) and learning rate (```alpha```). The learning rate ```alpha``` has to be randomly initialized between 0.0001 and 0.001. For the learning rate, you have to first set the last two digits of your roll number as the random seed using ```random.seed()``` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Xd8QFReNDyGs"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "## set the last two digits of your roll number as the random seed\n",
        "random_seed = None\n",
        "### START CODE HERE ###\n",
        "random_seed = 39\n",
        "### END CODE HERE ###\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "def initialize_parameters():\n",
        "    \"\"\"\n",
        "    This function randomly initializes the model parameters (w and b) and the hyperparameter alpha\n",
        "    Initial w and b should be randomly sampled from a normal distribution with mean 0\n",
        "    alpha should be randomly initialized between 0.0001 and 0.001 by using last two digits of your roll number as the random seed\n",
        "    Args:\n",
        "        None\n",
        "    Returns:\n",
        "        initial_w: Initial parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "        initial_b: Initial bias parameter (scalar) of the linear regression model\n",
        "        alpha: Learning rate\n",
        "    \"\"\"\n",
        "\n",
        "    initial_w = None\n",
        "    initial_b = None\n",
        "    alpha = None\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    random.seed(random_seed)\n",
        "    initial_w = np.random.normal(0, 1, X_train.shape[1])\n",
        "    initial_b = np.random.normal(0, 1, 1)\n",
        "    alpha = random.uniform(0.0001, 0.001)\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return initial_w,initial_b,alpha\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7nt5VdORUEu"
      },
      "source": [
        "In the next cell, the model is trained using batch gradient descent algorithm for ```num_iters=10000``` iterations. You can change the number of iterations to check any improvements in the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9rAYubtASozV",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after iteration 0 is 228253.03991210728\n",
            "Loss after iteration 100 is 204732.99774044217\n",
            "Loss after iteration 200 is 184322.5940496332\n",
            "Loss after iteration 300 is 166601.88602066136\n",
            "Loss after iteration 400 is 151207.7834825815\n",
            "Loss after iteration 500 is 137826.34983937227\n",
            "Loss after iteration 600 is 126186.14565309507\n",
            "Loss after iteration 700 is 116052.47367979353\n",
            "Loss after iteration 800 is 107222.40327731805\n",
            "Loss after iteration 900 is 99520.46863727551\n",
            "Loss after iteration 1000 is 92794.94958730959\n",
            "Loss after iteration 1100 is 86914.65606812606\n",
            "Loss after iteration 1200 is 81766.14807425837\n",
            "Loss after iteration 1300 is 77251.33208516595\n",
            "Loss after iteration 1400 is 73285.38299984002\n",
            "Loss after iteration 1500 is 69794.94749307401\n",
            "Loss after iteration 1600 is 66716.59068142022\n",
            "Loss after iteration 1700 is 63995.45314823594\n",
            "Loss after iteration 1800 is 61584.089839618726\n",
            "Loss after iteration 1900 is 59441.46620109445\n",
            "Loss after iteration 2000 is 57532.09026050272\n",
            "Loss after iteration 2100 is 55825.26224637776\n",
            "Loss after iteration 2200 is 54294.425824424645\n",
            "Loss after iteration 2300 is 52916.60719033325\n",
            "Loss after iteration 2400 is 51671.9301208822\n",
            "Loss after iteration 2500 is 50543.19669659942\n",
            "Loss after iteration 2600 is 49515.524802344575\n",
            "Loss after iteration 2700 is 48576.03471661499\n",
            "Loss after iteration 2800 is 47713.57814169861\n",
            "Loss after iteration 2900 is 46918.503927098885\n",
            "Loss after iteration 3000 is 46182.455517030336\n",
            "Loss after iteration 3100 is 45498.19582574618\n",
            "Loss after iteration 3200 is 44859.455826285775\n",
            "Loss after iteration 3300 is 44260.80364125746\n",
            "Loss after iteration 3400 is 43697.531359181936\n",
            "Loss after iteration 3500 is 43165.5571759276\n",
            "Loss after iteration 3600 is 42661.34078585762\n",
            "Loss after iteration 3700 is 42181.810228369184\n",
            "Loss after iteration 3800 is 41724.29863850433\n",
            "Loss after iteration 3900 is 41286.48956040073\n",
            "Loss after iteration 4000 is 40866.36966398927\n",
            "Loss after iteration 4100 is 40462.18786238454\n",
            "Loss after iteration 4200 is 40072.41996318617\n",
            "Loss after iteration 4300 is 39695.73810429395\n",
            "Loss after iteration 4400 is 39330.98432632759\n",
            "Loss after iteration 4500 is 38977.147721485075\n",
            "Loss after iteration 4600 is 38633.34467453502\n",
            "Loss after iteration 4700 is 38298.80177722536\n",
            "Loss after iteration 4800 is 37972.84105409538\n",
            "Loss after iteration 4900 is 37654.86718670459\n",
            "Loss after iteration 5000 is 37344.35646567688\n",
            "Loss after iteration 5100 is 37040.8472366057\n",
            "Loss after iteration 5200 is 36743.93163754808\n",
            "Loss after iteration 5300 is 36453.24845322869\n",
            "Loss after iteration 5400 is 36168.47693475707\n",
            "Loss after iteration 5500 is 35889.331454137246\n",
            "Loss after iteration 5600 is 35615.556880551005\n",
            "Loss after iteration 5700 is 35346.92458070201\n",
            "Loss after iteration 5800 is 35083.22895873949\n",
            "Loss after iteration 5900 is 34824.28446272138\n",
            "Loss after iteration 6000 is 34569.92299446738\n",
            "Loss after iteration 6100 is 34319.99166820467\n",
            "Loss after iteration 6200 is 34074.35087080157\n",
            "Loss after iteration 6300 is 33832.87258277746\n",
            "Loss after iteration 6400 is 33595.438924803304\n",
            "Loss after iteration 6500 is 33361.940899185596\n",
            "Loss after iteration 6600 is 33132.277299957335\n",
            "Loss after iteration 6700 is 32906.35376877118\n",
            "Loss after iteration 6800 is 32684.08197687824\n",
            "Loss after iteration 6900 is 32465.378916145084\n",
            "Loss after iteration 7000 is 32250.16628437039\n",
            "Loss after iteration 7100 is 32038.369952157507\n",
            "Loss after iteration 7200 is 31829.919500325457\n",
            "Loss after iteration 7300 is 31624.747818331707\n",
            "Loss after iteration 7400 is 31422.79075547037\n",
            "Loss after iteration 7500 is 31223.986817724202\n",
            "Loss after iteration 7600 is 31028.276904112907\n",
            "Loss after iteration 7700 is 30835.60407721381\n",
            "Loss after iteration 7800 is 30645.91336325134\n",
            "Loss after iteration 7900 is 30459.15157777523\n",
            "Loss after iteration 8000 is 30275.267173485372\n",
            "Loss after iteration 8100 is 30094.21010722774\n",
            "Loss after iteration 8200 is 29915.93172358778\n",
            "Loss after iteration 8300 is 29740.384652856148\n",
            "Loss after iteration 8400 is 29567.522721442736\n",
            "Loss after iteration 8500 is 29397.300873074746\n",
            "Loss after iteration 8600 is 29229.675099340173\n",
            "Loss after iteration 8700 is 29064.602378331936\n",
            "Loss after iteration 8800 is 28902.04062031668\n",
            "Loss after iteration 8900 is 28741.94861949724\n",
            "Loss after iteration 9000 is 28584.286011063807\n",
            "Loss after iteration 9100 is 28429.013232837315\n",
            "Loss after iteration 9200 is 28276.09149090265\n",
            "Loss after iteration 9300 is 28125.482728710762\n",
            "Loss after iteration 9400 is 27977.14959919851\n",
            "Loss after iteration 9500 is 27831.055439536372\n",
            "Loss after iteration 9600 is 27687.164248166402\n",
            "Loss after iteration 9700 is 27545.440663838242\n",
            "Loss after iteration 9800 is 27405.84994639037\n",
            "Loss after iteration 9900 is 27268.357959057652\n",
            "Updated w:  [ 0.58675338  4.11797934  0.45221292  1.58766937  1.23535395 11.23081008\n",
            "  2.52285585  2.4335428  -0.07749073 -0.02911519]\n",
            "Updated b:  [12.22492361]\n"
          ]
        }
      ],
      "source": [
        "# initialize the parameters and hyperparameter\n",
        "initial_w, initial_b, alpha = initialize_parameters()\n",
        "\n",
        "# number of iterations\n",
        "num_iters = 10000\n",
        "\n",
        "w,b,loss_hist = batch_gradient_descent(X_train ,y_train, initial_w, initial_b, alpha, num_iters)\n",
        "print(\"Updated w: \",w)\n",
        "print(\"Updated b: \",b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sluek2LkSozW"
      },
      "source": [
        "### 1.5. Final Train Error and Test Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH9oHC10fAlV"
      },
      "source": [
        "After the linear regression model is trained, we will compute the final train error and test error for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UIXb7sSvSozW",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Error:  27134.275306291987 , Test Error:  7497.161945126493\n"
          ]
        }
      ],
      "source": [
        "## Train and Test error computation\n",
        "\n",
        "train_error = loss_function(X_train,y_train,w,b)\n",
        "test_error = loss_function(X_test,y_test,w,b)\n",
        "print(\"Train Error: \",train_error, \", Test Error: \",test_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eximd12PbAgC"
      },
      "source": [
        "### 1.6. Plotting the loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meDwLCCIfOBo"
      },
      "source": [
        "We will plot the loss function values for every training iteration. If the model is trained properly, you will see that the loss function reduces as the training progesses and it converges at some point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DW4bue0oSozW"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# PLotting the loss values for every training iterations\n",
        "\n",
        "loss_plot = [loss_hist[i] for i in range(len(loss_hist))]\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss function\")\n",
        "plt.plot(loss_plot)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAVgq4HWCmDX"
      },
      "source": [
        "### 1.7 Experimenting with different values of the Hyperparemeters\n",
        "\n",
        "Previously, we randomly sampled the learning rate and the number of features to train the model. Now, you have to manually choose the number of features and the learning rate. Then, you have to train the model again on the manually choosen hyperparameters (number of features and learning rate). In the next cell, you have to manually choose the hyperparameters and write the code to train the model.\n",
        "\n",
        "After the model is trained, you have to compare the performance of the model with random chosen hyperparameters and manually chosen hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K2QuEbE1Clxq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after iteration 0 is 192172.01171372976\n",
            "Loss after iteration 100 is 173249.06375268934\n",
            "Loss after iteration 200 is 156821.6688211506\n",
            "Loss after iteration 300 is 142552.9092895095\n",
            "Loss after iteration 400 is 130151.4785469919\n",
            "Loss after iteration 500 is 119365.5043115431\n",
            "Loss after iteration 600 is 109977.20842507062\n",
            "Loss after iteration 700 is 101798.28985154435\n",
            "Loss after iteration 800 is 94665.93293694581\n",
            "Loss after iteration 900 is 88439.35625389937\n",
            "Loss after iteration 1000 is 82996.82882137658\n",
            "Loss after iteration 1100 is 78233.09040441083\n",
            "Loss after iteration 1200 is 74057.12117060713\n",
            "Loss after iteration 1300 is 70390.21339122369\n",
            "Loss after iteration 1400 is 67164.30428194528\n",
            "Loss after iteration 1500 is 64320.53461808574\n",
            "Loss after iteration 1600 is 61808.002548361954\n",
            "Loss after iteration 1700 is 59582.686172171714\n",
            "Loss after iteration 1800 is 57606.512025325486\n",
            "Loss after iteration 1900 is 55846.54971436743\n",
            "Loss after iteration 2000 is 54274.315615636064\n",
            "Loss after iteration 2100 is 52865.17086882655\n",
            "Loss after iteration 2200 is 51597.80089510159\n",
            "Loss after iteration 2300 is 50453.76539919143\n",
            "Loss after iteration 2400 is 49417.109310111366\n",
            "Loss after iteration 2500 is 48474.02640782261\n",
            "Loss after iteration 2600 is 47612.56850079626\n",
            "Loss after iteration 2700 is 46822.39398571466\n",
            "Loss after iteration 2800 is 46094.55045595997\n",
            "Loss after iteration 2900 is 45421.2867478183\n",
            "Loss after iteration 3000 is 44795.890437788956\n",
            "Loss after iteration 3100 is 44212.54734428272\n",
            "Loss after iteration 3200 is 43666.220053769255\n",
            "Loss after iteration 3300 is 43152.54289499739\n",
            "Loss after iteration 3400 is 42667.73113382143\n",
            "Loss after iteration 3500 is 42208.502462825294\n",
            "Loss after iteration 3600 is 41772.00912074096\n",
            "Loss after iteration 3700 is 41355.779202143945\n",
            "Loss after iteration 3800 is 40957.665912856304\n",
            "Loss after iteration 3900 is 40575.80369503603\n",
            "Loss after iteration 4000 is 40208.57029165237\n",
            "Loss after iteration 4100 is 39854.55394603432\n",
            "Loss after iteration 4200 is 39512.52504110358\n",
            "Loss after iteration 4300 is 39181.411577078106\n",
            "Loss after iteration 4400 is 38860.2779678512\n",
            "Loss after iteration 4500 is 38548.30670664516\n",
            "Loss after iteration 4600 is 38244.78251239903\n",
            "Loss after iteration 4700 is 37949.07862096792\n",
            "Loss after iteration 4800 is 37660.6449307037\n",
            "Loss after iteration 4900 is 37378.99775131873\n",
            "Loss after iteration 5000 is 37103.710938938806\n",
            "Loss after iteration 5100 is 36834.408229651424\n",
            "Loss after iteration 5200 is 36570.75660927377\n",
            "Loss after iteration 5300 is 36312.46057904083\n",
            "Loss after iteration 5400 is 36059.2571959136\n",
            "Loss after iteration 5500 is 35810.91178263437\n",
            "Loss after iteration 5600 is 35567.2142168582\n",
            "Loss after iteration 5700 is 35327.97572096826\n",
            "Loss after iteration 5800 is 35093.02608479887\n",
            "Loss after iteration 5900 is 34862.21126266813\n",
            "Loss after iteration 6000 is 34635.3912940576\n",
            "Loss after iteration 6100 is 34412.43850413678\n",
            "Loss after iteration 6200 is 34193.23594626176\n",
            "Loss after iteration 6300 is 33977.67605370591\n",
            "Loss after iteration 6400 is 33765.659472313855\n",
            "Loss after iteration 6500 is 33557.09404960362\n",
            "Loss after iteration 6600 is 33351.893959155714\n",
            "Loss after iteration 6700 is 33149.97894199352\n",
            "Loss after iteration 6800 is 32951.27364913659\n",
            "Loss after iteration 6900 is 32755.707071650268\n",
            "Loss after iteration 7000 is 32563.212046366796\n",
            "Loss after iteration 7100 is 32373.72482705422\n",
            "Loss after iteration 7200 is 32187.184712193357\n",
            "Loss after iteration 7300 is 32003.533721720138\n",
            "Loss after iteration 7400 is 31822.71631612488\n",
            "Loss after iteration 7500 is 31644.679152195196\n",
            "Loss after iteration 7600 is 31469.37087046206\n",
            "Loss after iteration 7700 is 31296.741910077624\n",
            "Loss after iteration 7800 is 31126.744347431355\n",
            "Loss after iteration 7900 is 30959.331755310937\n",
            "Loss after iteration 8000 is 30794.45907984653\n",
            "Loss after iteration 8100 is 30632.08253285062\n",
            "Loss after iteration 8200 is 30472.159497488825\n",
            "Loss after iteration 8300 is 30314.648445496012\n",
            "Loss after iteration 8400 is 30159.508864393858\n",
            "Loss after iteration 8500 is 30006.70119337493\n",
            "Loss after iteration 8600 is 29856.186766698225\n",
            "Loss after iteration 8700 is 29707.927763597767\n",
            "Loss after iteration 8800 is 29561.887163840707\n",
            "Loss after iteration 8900 is 29418.02870818766\n",
            "Loss after iteration 9000 is 29276.31686310953\n",
            "Loss after iteration 9100 is 29136.71678920153\n",
            "Loss after iteration 9200 is 28999.194312811273\n",
            "Loss after iteration 9300 is 28863.715900462394\n",
            "Loss after iteration 9400 is 28730.248635711916\n",
            "Loss after iteration 9500 is 28598.760198128162\n",
            "Loss after iteration 9600 is 28469.218844118248\n",
            "Loss after iteration 9700 is 28341.59338937048\n",
            "Loss after iteration 9800 is 28215.853192708724\n",
            "Loss after iteration 9900 is 28091.968141182842\n",
            "Loss after iteration 10000 is 27969.908636243046\n",
            "Loss after iteration 10100 is 27849.645580866196\n",
            "Loss after iteration 10200 is 27731.15036751993\n",
            "Loss after iteration 10300 is 27614.3948668654\n",
            "Loss after iteration 10400 is 27499.351417113125\n",
            "Loss after iteration 10500 is 27385.99281395709\n",
            "Loss after iteration 10600 is 27274.29230102295\n",
            "Loss after iteration 10700 is 27164.223560773968\n",
            "Loss after iteration 10800 is 27055.7607058262\n",
            "Loss after iteration 10900 is 26948.878270630477\n",
            "Loss after iteration 11000 is 26843.551203484574\n",
            "Loss after iteration 11100 is 26739.754858843455\n",
            "Loss after iteration 11200 is 26637.464989899705\n",
            "Loss after iteration 11300 is 26536.65774140999\n",
            "Loss after iteration 11400 is 26437.309642746077\n",
            "Loss after iteration 11500 is 26339.39760115226\n",
            "Loss after iteration 11600 is 26242.89889519268\n",
            "Loss after iteration 11700 is 26147.791168374442\n",
            "Loss after iteration 11800 is 26054.052422934095\n",
            "Loss after iteration 11900 is 25961.661013776553\n",
            "Loss after iteration 12000 is 25870.595642556676\n",
            "Loss after iteration 12100 is 25780.835351895148\n",
            "Loss after iteration 12200 is 25692.359519721016\n",
            "Loss after iteration 12300 is 25605.147853734226\n",
            "Loss after iteration 12400 is 25519.18038598226\n",
            "Loss after iteration 12500 is 25434.437467545555\n",
            "Loss after iteration 12600 is 25350.899763326794\n",
            "Loss after iteration 12700 is 25268.548246940118\n",
            "Loss after iteration 12800 is 25187.364195696115\n",
            "Loss after iteration 12900 is 25107.32918567928\n",
            "Loss after iteration 13000 is 25028.425086914725\n",
            "Loss after iteration 13100 is 24950.634058621305\n",
            "Loss after iteration 13200 is 24873.93854454841\n",
            "Loss after iteration 13300 is 24798.321268394204\n",
            "Loss after iteration 13400 is 24723.765229302837\n",
            "Loss after iteration 13500 is 24650.253697438726\n",
            "Loss after iteration 13600 is 24577.770209635917\n",
            "Loss after iteration 13700 is 24506.29856512067\n",
            "Loss after iteration 13800 is 24435.822821305694\n",
            "Loss after iteration 13900 is 24366.327289654313\n",
            "Loss after iteration 14000 is 24297.796531613123\n",
            "Loss after iteration 14100 is 24230.215354611682\n",
            "Loss after iteration 14200 is 24163.568808127937\n",
            "Loss after iteration 14300 is 24097.842179817886\n",
            "Loss after iteration 14400 is 24033.020991708523\n",
            "Loss after iteration 14500 is 23969.090996452676\n",
            "Loss after iteration 14600 is 23906.038173644538\n",
            "Loss after iteration 14700 is 23843.848726195\n",
            "Loss after iteration 14800 is 23782.509076765466\n",
            "Loss after iteration 14900 is 23722.005864259336\n",
            "Loss after iteration 15000 is 23662.325940369883\n",
            "Loss after iteration 15100 is 23603.45636618391\n",
            "Loss after iteration 15200 is 23545.384408839753\n",
            "Loss after iteration 15300 is 23488.097538239213\n",
            "Loss after iteration 15400 is 23431.583423812084\n",
            "Loss after iteration 15500 is 23375.82993133267\n",
            "Loss after iteration 15600 is 23320.825119787252\n",
            "Loss after iteration 15700 is 23266.557238291774\n",
            "Loss after iteration 15800 is 23213.01472305886\n",
            "Loss after iteration 15900 is 23160.186194413385\n",
            "Loss after iteration 16000 is 23108.060453855698\n",
            "Loss after iteration 16100 is 23056.626481171927\n",
            "Loss after iteration 16200 is 23005.873431590386\n",
            "Loss after iteration 16300 is 22955.79063298345\n",
            "Loss after iteration 16400 is 22906.367583114177\n",
            "Loss after iteration 16500 is 22857.59394692678\n",
            "Loss after iteration 16600 is 22809.45955388056\n",
            "Loss after iteration 16700 is 22761.954395326244\n",
            "Loss after iteration 16800 is 22715.068621924307\n",
            "Loss after iteration 16900 is 22668.79254110448\n",
            "Loss after iteration 17000 is 22623.1166145658\n",
            "Loss after iteration 17100 is 22578.031455816577\n",
            "Loss after iteration 17200 is 22533.527827753605\n",
            "Loss after iteration 17300 is 22489.59664028001\n",
            "Loss after iteration 17400 is 22446.228947961106\n",
            "Loss after iteration 17500 is 22403.415947717684\n",
            "Loss after iteration 17600 is 22361.148976556015\n",
            "Loss after iteration 17700 is 22319.41950933422\n",
            "Loss after iteration 17800 is 22278.219156564126\n",
            "Loss after iteration 17900 is 22237.539662248273\n",
            "Loss after iteration 18000 is 22197.372901751383\n",
            "Loss after iteration 18100 is 22157.710879705843\n",
            "Loss after iteration 18200 is 22118.545727950528\n",
            "Loss after iteration 18300 is 22079.869703502554\n",
            "Loss after iteration 18400 is 22041.67518656137\n",
            "Loss after iteration 18500 is 22003.954678544633\n",
            "Loss after iteration 18600 is 21966.700800155544\n",
            "Loss after iteration 18700 is 21929.9062894809\n",
            "Loss after iteration 18800 is 21893.564000119462\n",
            "Loss after iteration 18900 is 21857.666899340293\n",
            "Loss after iteration 19000 is 21822.208066270374\n",
            "Loss after iteration 19100 is 21787.18069011113\n",
            "Loss after iteration 19200 is 21752.578068383496\n",
            "Loss after iteration 19300 is 21718.393605200876\n",
            "Loss after iteration 19400 is 21684.62080956962\n",
            "Loss after iteration 19500 is 21651.253293716763\n",
            "Loss after iteration 19600 is 21618.28477144426\n",
            "Loss after iteration 19700 is 21585.709056509615\n",
            "Loss after iteration 19800 is 21553.52006103222\n",
            "Loss after iteration 19900 is 21521.711793925184\n",
            "Loss after iteration 20000 is 21490.27835935215\n",
            "Loss after iteration 20100 is 21459.213955208717\n",
            "Loss after iteration 20200 is 21428.51287162811\n",
            "Loss after iteration 20300 is 21398.169489510656\n",
            "Loss after iteration 20400 is 21368.17827907671\n",
            "Loss after iteration 20500 is 21338.533798442688\n",
            "Loss after iteration 20600 is 21309.23069221981\n",
            "Loss after iteration 20700 is 21280.26369013513\n",
            "Loss after iteration 20800 is 21251.627605674643\n",
            "Loss after iteration 20900 is 21223.317334747993\n",
            "Loss after iteration 21000 is 21195.327854374475\n",
            "Loss after iteration 21100 is 21167.65422138998\n",
            "Loss after iteration 21200 is 21140.291571174632\n",
            "Loss after iteration 21300 is 21113.235116400603\n",
            "Loss after iteration 21400 is 21086.480145800007\n",
            "Loss after iteration 21500 is 21060.0220229524\n",
            "Loss after iteration 21600 is 21033.85618509159\n",
            "Loss after iteration 21700 is 21007.978141931526\n",
            "Loss after iteration 21800 is 20982.383474510894\n",
            "Loss after iteration 21900 is 20957.067834056143\n",
            "Loss after iteration 22000 is 20932.026940862594\n",
            "Loss after iteration 22100 is 20907.25658319349\n",
            "Loss after iteration 22200 is 20882.752616196547\n",
            "Loss after iteration 22300 is 20858.510960837753\n",
            "Loss after iteration 22400 is 20834.527602852257\n",
            "Loss after iteration 22500 is 20810.79859171191\n",
            "Loss after iteration 22600 is 20787.320039609313\n",
            "Loss after iteration 22700 is 20764.08812045805\n",
            "Loss after iteration 22800 is 20741.09906890891\n",
            "Loss after iteration 22900 is 20718.349179381694\n",
            "Loss after iteration 23000 is 20695.8348051126\n",
            "Loss after iteration 23100 is 20673.55235721671\n",
            "Loss after iteration 23200 is 20651.498303765424\n",
            "Loss after iteration 23300 is 20629.669168878696\n",
            "Loss after iteration 23400 is 20608.061531831598\n",
            "Loss after iteration 23500 is 20586.672026175314\n",
            "Loss after iteration 23600 is 20565.497338872003\n",
            "Loss after iteration 23700 is 20544.53420944352\n",
            "Loss after iteration 23800 is 20523.779429133723\n",
            "Loss after iteration 23900 is 20503.229840084117\n",
            "Loss after iteration 24000 is 20482.88233452267\n",
            "Loss after iteration 24100 is 20462.733853965532\n",
            "Loss after iteration 24200 is 20442.781388431486\n",
            "Loss after iteration 24300 is 20423.021975669\n",
            "Loss after iteration 24400 is 20403.45270039548\n",
            "Loss after iteration 24500 is 20384.07069354878\n",
            "Loss after iteration 24600 is 20364.873131550554\n",
            "Loss after iteration 24700 is 20345.857235581436\n",
            "Loss after iteration 24800 is 20327.02027086769\n",
            "Loss after iteration 24900 is 20308.359545979343\n",
            "Loss after iteration 25000 is 20289.872412139386\n",
            "Loss after iteration 25100 is 20271.55626254411\n",
            "Loss after iteration 25200 is 20253.40853169423\n",
            "Loss after iteration 25300 is 20235.426694736645\n",
            "Loss after iteration 25400 is 20217.608266816744\n",
            "Loss after iteration 25500 is 20199.950802441028\n",
            "Loss after iteration 25600 is 20182.451894849888\n",
            "Loss after iteration 25700 is 20165.109175400394\n",
            "Loss after iteration 25800 is 20147.920312958915\n",
            "Loss after iteration 25900 is 20130.883013303443\n",
            "Loss after iteration 26000 is 20113.995018535417\n",
            "Loss after iteration 26100 is 20097.254106500994\n",
            "Loss after iteration 26200 is 20080.658090221455\n",
            "Loss after iteration 26300 is 20064.204817332775\n",
            "Loss after iteration 26400 is 20047.89216953408\n",
            "Loss after iteration 26500 is 20031.718062044944\n",
            "Loss after iteration 26600 is 20015.680443071284\n",
            "Loss after iteration 26700 is 19999.777293279825\n",
            "Loss after iteration 26800 is 19984.00662528091\n",
            "Loss after iteration 26900 is 19968.36648311959\n",
            "Loss after iteration 27000 is 19952.854941774793\n",
            "Loss after iteration 27100 is 19937.470106666526\n",
            "Loss after iteration 27200 is 19922.210113170873\n",
            "Loss after iteration 27300 is 19907.07312614279\n",
            "Loss after iteration 27400 is 19892.05733944645\n",
            "Loss after iteration 27500 is 19877.16097549317\n",
            "Loss after iteration 27600 is 19862.382284786578\n",
            "Loss after iteration 27700 is 19847.719545475156\n",
            "Loss after iteration 27800 is 19833.17106291182\n",
            "Loss after iteration 27900 is 19818.735169220618\n",
            "Loss after iteration 28000 is 19804.41022287023\n",
            "Loss after iteration 28100 is 19790.194608254365\n",
            "Loss after iteration 28200 is 19776.086735278812\n",
            "Loss after iteration 28300 is 19762.085038955054\n",
            "Loss after iteration 28400 is 19748.187979000395\n",
            "Loss after iteration 28500 is 19734.39403944446\n",
            "Loss after iteration 28600 is 19720.701728241947\n",
            "Loss after iteration 28700 is 19707.109576891566\n",
            "Loss after iteration 28800 is 19693.616140061073\n",
            "Loss after iteration 28900 is 19680.21999521827\n",
            "Loss after iteration 29000 is 19666.919742267877\n",
            "Loss after iteration 29100 is 19653.71400319419\n",
            "Loss after iteration 29200 is 19640.601421709478\n",
            "Loss after iteration 29300 is 19627.580662907953\n",
            "Loss after iteration 29400 is 19614.65041292526\n",
            "Loss after iteration 29500 is 19601.809378603408\n",
            "Loss after iteration 29600 is 19589.056287161024\n",
            "Loss after iteration 29700 is 19576.389885868906\n",
            "Loss after iteration 29800 is 19563.808941730684\n",
            "Loss after iteration 29900 is 19551.31224116865\n",
            "Loss after iteration 30000 is 19538.898589714536\n",
            "Loss after iteration 30100 is 19526.566811705256\n",
            "Loss after iteration 30200 is 19514.31574998349\n",
            "Loss after iteration 30300 is 19502.14426560302\n",
            "Loss after iteration 30400 is 19490.05123753883\n",
            "Loss after iteration 30500 is 19478.03556240174\n",
            "Loss after iteration 30600 is 19466.09615415766\n",
            "Loss after iteration 30700 is 19454.231943851304\n",
            "Loss after iteration 30800 is 19442.441879334263\n",
            "Loss after iteration 30900 is 19430.724924997496\n",
            "Loss after iteration 31000 is 19419.08006150802\n",
            "Loss after iteration 31100 is 19407.50628554981\n",
            "Loss after iteration 31200 is 19396.002609568874\n",
            "Loss after iteration 31300 is 19384.568061522332\n",
            "Loss after iteration 31400 is 19373.201684631546\n",
            "Loss after iteration 31500 is 19361.90253713913\n",
            "Loss after iteration 31600 is 19350.669692069896\n",
            "Loss after iteration 31700 is 19339.5022369956\n",
            "Loss after iteration 31800 is 19328.39927380337\n",
            "Loss after iteration 31900 is 19317.35991846794\n",
            "Loss after iteration 32000 is 19306.383300827398\n",
            "Loss after iteration 32100 is 19295.468564362618\n",
            "Loss after iteration 32200 is 19284.61486598012\n",
            "Loss after iteration 32300 is 19273.82137579844\n",
            "Loss after iteration 32400 is 19263.087276937917\n",
            "Loss after iteration 32500 is 19252.41176531378\n",
            "Loss after iteration 32600 is 19241.79404943256\n",
            "Loss after iteration 32700 is 19231.233350191786\n",
            "Loss after iteration 32800 is 19220.728900682825\n",
            "Loss after iteration 32900 is 19210.279945996866\n",
            "Loss after iteration 33000 is 19199.885743034036\n",
            "Loss after iteration 33100 is 19189.545560315506\n",
            "Loss after iteration 33200 is 19179.258677798665\n",
            "Loss after iteration 33300 is 19169.02438669515\n",
            "Loss after iteration 33400 is 19158.841989291865\n",
            "Loss after iteration 33500 is 19148.710798774788\n",
            "Loss after iteration 33600 is 19138.630139055622\n",
            "Loss after iteration 33700 is 19128.599344601207\n",
            "Loss after iteration 33800 is 19118.61776026564\n",
            "Loss after iteration 33900 is 19108.68474112505\n",
            "Loss after iteration 34000 is 19098.799652315098\n",
            "Loss after iteration 34100 is 19088.961868870927\n",
            "Loss after iteration 34200 is 19079.170775569808\n",
            "Loss after iteration 34300 is 19069.42576677617\n",
            "Loss after iteration 34400 is 19059.726246289196\n",
            "Loss after iteration 34500 is 19050.071627192785\n",
            "Loss after iteration 34600 is 19040.461331707957\n",
            "Loss after iteration 34700 is 19030.894791047547\n",
            "Loss after iteration 34800 is 19021.371445273297\n",
            "Loss after iteration 34900 is 19011.890743155134\n",
            "Loss after iteration 35000 is 19002.452142032784\n",
            "Loss after iteration 35100 is 18993.055107679516\n",
            "Loss after iteration 35200 is 18983.699114168117\n",
            "Loss after iteration 35300 is 18974.38364373894\n",
            "Loss after iteration 35400 is 18965.108186670128\n",
            "Loss after iteration 35500 is 18955.872241149842\n",
            "Loss after iteration 35600 is 18946.675313150572\n",
            "Loss after iteration 35700 is 18937.516916305423\n",
            "Loss after iteration 35800 is 18928.396571786398\n",
            "Loss after iteration 35900 is 18919.313808184586\n",
            "Loss after iteration 36000 is 18910.268161392294\n",
            "Loss after iteration 36100 is 18901.25917448704\n",
            "Loss after iteration 36200 is 18892.286397617412\n",
            "Loss after iteration 36300 is 18883.34938789071\n",
            "Loss after iteration 36400 is 18874.447709262422\n",
            "Loss after iteration 36500 is 18865.58093242744\n",
            "Loss after iteration 36600 is 18856.74863471301\n",
            "Loss after iteration 36700 is 18847.950399973382\n",
            "Loss after iteration 36800 is 18839.185818486167\n",
            "Loss after iteration 36900 is 18830.454486850318\n",
            "Loss after iteration 37000 is 18821.756007885724\n",
            "Loss after iteration 37100 is 18813.08999053448\n",
            "Loss after iteration 37200 is 18804.456049763627\n",
            "Loss after iteration 37300 is 18795.853806469502\n",
            "Loss after iteration 37400 is 18787.28288738363\n",
            "Loss after iteration 37500 is 18778.742924980063\n",
            "Loss after iteration 37600 is 18770.233557384214\n",
            "Loss after iteration 37700 is 18761.754428283188\n",
            "Loss after iteration 37800 is 18753.305186837475\n",
            "Loss after iteration 37900 is 18744.88548759408\n",
            "Loss after iteration 38000 is 18736.494990401072\n",
            "Loss after iteration 38100 is 18728.1333603234\n",
            "Loss after iteration 38200 is 18719.800267560197\n",
            "Loss after iteration 38300 is 18711.495387363222\n",
            "Loss after iteration 38400 is 18703.21839995678\n",
            "Loss after iteration 38500 is 18694.968990458776\n",
            "Loss after iteration 38600 is 18686.746848803126\n",
            "Loss after iteration 38700 is 18678.551669663335\n",
            "Loss after iteration 38800 is 18670.383152377348\n",
            "Loss after iteration 38900 is 18662.241000873528\n",
            "Loss after iteration 39000 is 18654.124923597934\n",
            "Loss after iteration 39100 is 18646.034633442607\n",
            "Loss after iteration 39200 is 18637.969847675115\n",
            "Loss after iteration 39300 is 18629.930287869174\n",
            "Loss after iteration 39400 is 18621.91567983637\n",
            "Loss after iteration 39500 is 18613.925753558993\n",
            "Loss after iteration 39600 is 18605.960243123907\n",
            "Loss after iteration 39700 is 18598.01888665752\n",
            "Loss after iteration 39800 is 18590.10142626174\n",
            "Loss after iteration 39900 is 18582.207607951\n",
            "Loss after iteration 40000 is 18574.337181590246\n",
            "Loss after iteration 40100 is 18566.489900833934\n",
            "Loss after iteration 40200 is 18558.66552306598\n",
            "Loss after iteration 40300 is 18550.863809340717\n",
            "Loss after iteration 40400 is 18543.084524324713\n",
            "Loss after iteration 40500 is 18535.32743623959\n",
            "Loss after iteration 40600 is 18527.59231680573\n",
            "Loss after iteration 40700 is 18519.878941186835\n",
            "Loss after iteration 40800 is 18512.187087935432\n",
            "Loss after iteration 40900 is 18504.516538939224\n",
            "Loss after iteration 41000 is 18496.86707936828\n",
            "Loss after iteration 41100 is 18489.23849762309\n",
            "Loss after iteration 41200 is 18481.630585283434\n",
            "Loss after iteration 41300 is 18474.043137058055\n",
            "Loss after iteration 41400 is 18466.47595073517\n",
            "Loss after iteration 41500 is 18458.92882713373\n",
            "Loss after iteration 41600 is 18451.40157005547\n",
            "Loss after iteration 41700 is 18443.89398623774\n",
            "Loss after iteration 41800 is 18436.40588530706\n",
            "Loss after iteration 41900 is 18428.93707973344\n",
            "Loss after iteration 42000 is 18421.487384785407\n",
            "Loss after iteration 42100 is 18414.056618485745\n",
            "Loss after iteration 42200 is 18406.644601567965\n",
            "Loss after iteration 42300 is 18399.251157433468\n",
            "Loss after iteration 42400 is 18391.87611210935\n",
            "Loss after iteration 42500 is 18384.51929420692\n",
            "Loss after iteration 42600 is 18377.180534880878\n",
            "Loss after iteration 42700 is 18369.85966778909\n",
            "Loss after iteration 42800 is 18362.556529053087\n",
            "Loss after iteration 42900 is 18355.27095721913\n",
            "Loss after iteration 43000 is 18348.00279321992\n",
            "Loss after iteration 43100 is 18340.751880336888\n",
            "Loss after iteration 43200 is 18333.51806416316\n",
            "Loss after iteration 43300 is 18326.301192567014\n",
            "Loss after iteration 43400 is 18319.101115655994\n",
            "Loss after iteration 43500 is 18311.91768574155\n",
            "Loss after iteration 43600 is 18304.750757304282\n",
            "Loss after iteration 43700 is 18297.600186959695\n",
            "Loss after iteration 43800 is 18290.46583342453\n",
            "Loss after iteration 43900 is 18283.347557483597\n",
            "Loss after iteration 44000 is 18276.24522195721\n",
            "Loss after iteration 44100 is 18269.158691669032\n",
            "Loss after iteration 44200 is 18262.08783341453\n",
            "Loss after iteration 44300 is 18255.032515929866\n",
            "Loss after iteration 44400 is 18247.99260986133\n",
            "Loss after iteration 44500 is 18240.967987735203\n",
            "Loss after iteration 44600 is 18233.958523928195\n",
            "Loss after iteration 44700 is 18226.964094638242\n",
            "Loss after iteration 44800 is 18219.984577855852\n",
            "Loss after iteration 44900 is 18213.019853335827\n",
            "Loss after iteration 45000 is 18206.069802569546\n",
            "Loss after iteration 45100 is 18199.13430875761\n",
            "Loss after iteration 45200 is 18192.213256782878\n",
            "Loss after iteration 45300 is 18185.3065331841\n",
            "Loss after iteration 45400 is 18178.41402612975\n",
            "Loss after iteration 45500 is 18171.535625392477\n",
            "Loss after iteration 45600 is 18164.671222323825\n",
            "Loss after iteration 45700 is 18157.820709829422\n",
            "Loss after iteration 45800 is 18150.983982344536\n",
            "Loss after iteration 45900 is 18144.160935810047\n",
            "Loss after iteration 46000 is 18137.351467648758\n",
            "Loss after iteration 46100 is 18130.555476742138\n",
            "Loss after iteration 46200 is 18123.772863407394\n",
            "Loss after iteration 46300 is 18117.003529374884\n",
            "Loss after iteration 46400 is 18110.247377766005\n",
            "Loss after iteration 46500 is 18103.5043130713\n",
            "Loss after iteration 46600 is 18096.774241128947\n",
            "Loss after iteration 46700 is 18090.05706910367\n",
            "Loss after iteration 46800 is 18083.35270546588\n",
            "Loss after iteration 46900 is 18076.661059971204\n",
            "Loss after iteration 47000 is 18069.982043640335\n",
            "Loss after iteration 47100 is 18063.31556873921\n",
            "Loss after iteration 47200 is 18056.66154875945\n",
            "Loss after iteration 47300 is 18050.01989839918\n",
            "Loss after iteration 47400 is 18043.390533544123\n",
            "Loss after iteration 47500 is 18036.773371249008\n",
            "Loss after iteration 47600 is 18030.16832971922\n",
            "Loss after iteration 47700 is 18023.575328292845\n",
            "Loss after iteration 47800 is 18016.994287422865\n",
            "Loss after iteration 47900 is 18010.42512865979\n",
            "Loss after iteration 48000 is 18003.86777463444\n",
            "Loss after iteration 48100 is 17997.32214904105\n",
            "Loss after iteration 48200 is 17990.788176620663\n",
            "Loss after iteration 48300 is 17984.265783144747\n",
            "Loss after iteration 48400 is 17977.754895399095\n",
            "Loss after iteration 48500 is 17971.255441167985\n",
            "Loss after iteration 48600 is 17964.767349218575\n",
            "Loss after iteration 48700 is 17958.29054928557\n",
            "Loss after iteration 48800 is 17951.824972056085\n",
            "Loss after iteration 48900 is 17945.370549154824\n",
            "Loss after iteration 49000 is 17938.927213129413\n",
            "Loss after iteration 49100 is 17932.494897436045\n",
            "Loss after iteration 49200 is 17926.07353642527\n",
            "Loss after iteration 49300 is 17919.6630653281\n",
            "Loss after iteration 49400 is 17913.26342024224\n",
            "Loss after iteration 49500 is 17906.874538118638\n",
            "Loss after iteration 49600 is 17900.49635674817\n",
            "Loss after iteration 49700 is 17894.128814748547\n",
            "Loss after iteration 49800 is 17887.771851551475\n",
            "Loss after iteration 49900 is 17881.425407389994\n",
            "Loss after iteration 50000 is 17875.089423286\n",
            "Loss after iteration 50100 is 17868.763841037984\n",
            "Loss after iteration 50200 is 17862.44860320896\n",
            "Loss after iteration 50300 is 17856.1436531146\n",
            "Loss after iteration 50400 is 17849.848934811536\n",
            "Loss after iteration 50500 is 17843.56439308585\n",
            "Loss after iteration 50600 is 17837.289973441766\n",
            "Loss after iteration 50700 is 17831.0256220905\n",
            "Loss after iteration 50800 is 17824.77128593931\n",
            "Loss after iteration 50900 is 17818.526912580684\n",
            "Loss after iteration 51000 is 17812.29245028174\n",
            "Loss after iteration 51100 is 17806.06784797377\n",
            "Loss after iteration 51200 is 17799.853055241943\n",
            "Loss after iteration 51300 is 17793.648022315218\n",
            "Loss after iteration 51400 is 17787.45270005635\n",
            "Loss after iteration 51500 is 17781.2670399521\n",
            "Loss after iteration 51600 is 17775.09099410359\n",
            "Loss after iteration 51700 is 17768.924515216826\n",
            "Loss after iteration 51800 is 17762.76755659331\n",
            "Loss after iteration 51900 is 17756.620072120917\n",
            "Loss after iteration 52000 is 17750.482016264756\n",
            "Loss after iteration 52100 is 17744.35334405834\n",
            "Loss after iteration 52200 is 17738.234011094803\n",
            "Loss after iteration 52300 is 17732.12397351825\n",
            "Loss after iteration 52400 is 17726.023188015304\n",
            "Loss after iteration 52500 is 17719.93161180673\n",
            "Loss after iteration 52600 is 17713.84920263922\n",
            "Loss after iteration 52700 is 17707.77591877731\n",
            "Loss after iteration 52800 is 17701.711718995422\n",
            "Loss after iteration 52900 is 17695.656562570017\n",
            "Loss after iteration 53000 is 17689.61040927186\n",
            "Loss after iteration 53100 is 17683.573219358495\n",
            "Loss after iteration 53200 is 17677.54495356673\n",
            "Loss after iteration 53300 is 17671.52557310528\n",
            "Loss after iteration 53400 is 17665.515039647562\n",
            "Loss after iteration 53500 is 17659.513315324584\n",
            "Loss after iteration 53600 is 17653.520362717867\n",
            "Loss after iteration 53700 is 17647.536144852653\n",
            "Loss after iteration 53800 is 17641.56062519104\n",
            "Loss after iteration 53900 is 17635.59376762535\n",
            "Loss after iteration 54000 is 17629.635536471527\n",
            "Loss after iteration 54100 is 17623.68589646268\n",
            "Loss after iteration 54200 is 17617.74481274272\n",
            "Loss after iteration 54300 is 17611.8122508601\n",
            "Loss after iteration 54400 is 17605.888176761633\n",
            "Loss after iteration 54500 is 17599.97255678643\n",
            "Loss after iteration 54600 is 17594.065357659936\n",
            "Loss after iteration 54700 is 17588.166546488057\n",
            "Loss after iteration 54800 is 17582.27609075135\n",
            "Loss after iteration 54900 is 17576.393958299344\n",
            "Loss after iteration 55000 is 17570.520117344968\n",
            "Loss after iteration 55100 is 17564.654536458977\n",
            "Loss after iteration 55200 is 17558.797184564573\n",
            "Loss after iteration 55300 is 17552.94803093205\n",
            "Loss after iteration 55400 is 17547.10704517354\n",
            "Loss after iteration 55500 is 17541.274197237813\n",
            "Loss after iteration 55600 is 17535.44945740523\n",
            "Loss after iteration 55700 is 17529.632796282716\n",
            "Loss after iteration 55800 is 17523.824184798803\n",
            "Loss after iteration 55900 is 17518.02359419882\n",
            "Loss after iteration 56000 is 17512.23099604008\n",
            "Loss after iteration 56100 is 17506.4463621872\n",
            "Loss after iteration 56200 is 17500.669664807454\n",
            "Loss after iteration 56300 is 17494.900876366246\n",
            "Loss after iteration 56400 is 17489.13996962262\n",
            "Loss after iteration 56500 is 17483.38691762482\n",
            "Loss after iteration 56600 is 17477.64169370599\n",
            "Loss after iteration 56700 is 17471.904271479885\n",
            "Loss after iteration 56800 is 17466.174624836665\n",
            "Loss after iteration 56900 is 17460.45272793876\n",
            "Loss after iteration 57000 is 17454.738555216773\n",
            "Loss after iteration 57100 is 17449.032081365505\n",
            "Loss after iteration 57200 is 17443.33328134\n",
            "Loss after iteration 57300 is 17437.64213035166\n",
            "Loss after iteration 57400 is 17431.958603864412\n",
            "Loss after iteration 57500 is 17426.28267759095\n",
            "Loss after iteration 57600 is 17420.61432748905\n",
            "Loss after iteration 57700 is 17414.953529757906\n",
            "Loss after iteration 57800 is 17409.300260834563\n",
            "Loss after iteration 57900 is 17403.65449739036\n",
            "Loss after iteration 58000 is 17398.01621632749\n",
            "Loss after iteration 58100 is 17392.385394775534\n",
            "Loss after iteration 58200 is 17386.76201008816\n",
            "Loss after iteration 58300 is 17381.14603983974\n",
            "Loss after iteration 58400 is 17375.537461822154\n",
            "Loss after iteration 58500 is 17369.936254041528\n",
            "Loss after iteration 58600 is 17364.34239471512\n",
            "Loss after iteration 58700 is 17358.755862268168\n",
            "Loss after iteration 58800 is 17353.176635330885\n",
            "Loss after iteration 58900 is 17347.604692735382\n",
            "Loss after iteration 59000 is 17342.04001351277\n",
            "Loss after iteration 59100 is 17336.482576890194\n",
            "Loss after iteration 59200 is 17330.93236228797\n",
            "Loss after iteration 59300 is 17325.389349316793\n",
            "Loss after iteration 59400 is 17319.85351777493\n",
            "Loss after iteration 59500 is 17314.324847645454\n",
            "Loss after iteration 59600 is 17308.803319093608\n",
            "Loss after iteration 59700 is 17303.288912464122\n",
            "Loss after iteration 59800 is 17297.78160827858\n",
            "Loss after iteration 59900 is 17292.281387232895\n",
            "Loss after iteration 60000 is 17286.788230194757\n",
            "Loss after iteration 60100 is 17281.302118201143\n",
            "Loss after iteration 60200 is 17275.823032455868\n",
            "Loss after iteration 60300 is 17270.350954327172\n",
            "Loss after iteration 60400 is 17264.88586534535\n",
            "Loss after iteration 60500 is 17259.427747200432\n",
            "Loss after iteration 60600 is 17253.976581739837\n",
            "Loss after iteration 60700 is 17248.532350966154\n",
            "Loss after iteration 60800 is 17243.095037034887\n",
            "Loss after iteration 60900 is 17237.664622252305\n",
            "Loss after iteration 61000 is 17232.2410890732\n",
            "Loss after iteration 61100 is 17226.824420098856\n",
            "Loss after iteration 61200 is 17221.414598074887\n",
            "Loss after iteration 61300 is 17216.011605889224\n",
            "Loss after iteration 61400 is 17210.61542657005\n",
            "Loss after iteration 61500 is 17205.226043283827\n",
            "Loss after iteration 61600 is 17199.843439333345\n",
            "Loss after iteration 61700 is 17194.46759815576\n",
            "Loss after iteration 61800 is 17189.09850332071\n",
            "Loss after iteration 61900 is 17183.736138528453\n",
            "Loss after iteration 62000 is 17178.38048760797\n",
            "Loss after iteration 62100 is 17173.03153451522\n",
            "Loss after iteration 62200 is 17167.689263331325\n",
            "Loss after iteration 62300 is 17162.3536582608\n",
            "Loss after iteration 62400 is 17157.02470362981\n",
            "Loss after iteration 62500 is 17151.702383884527\n",
            "Loss after iteration 62600 is 17146.386683589386\n",
            "Loss after iteration 62700 is 17141.077587425476\n",
            "Loss after iteration 62800 is 17135.77508018889\n",
            "Loss after iteration 62900 is 17130.47914678915\n",
            "Loss after iteration 63000 is 17125.189772247588\n",
            "Loss after iteration 63100 is 17119.90694169587\n",
            "Loss after iteration 63200 is 17114.630640374402\n",
            "Loss after iteration 63300 is 17109.36085363085\n",
            "Loss after iteration 63400 is 17104.097566918663\n",
            "Loss after iteration 63500 is 17098.840765795634\n",
            "Loss after iteration 63600 is 17093.590435922422\n",
            "Loss after iteration 63700 is 17088.346563061194\n",
            "Loss after iteration 63800 is 17083.109133074206\n",
            "Loss after iteration 63900 is 17077.878131922444\n",
            "Loss after iteration 64000 is 17072.653545664263\n",
            "Loss after iteration 64100 is 17067.435360454103\n",
            "Loss after iteration 64200 is 17062.223562541127\n",
            "Loss after iteration 64300 is 17057.018138267984\n",
            "Loss after iteration 64400 is 17051.81907406952\n",
            "Loss after iteration 64500 is 17046.626356471563\n",
            "Loss after iteration 64600 is 17041.439972089647\n",
            "Loss after iteration 64700 is 17036.259907627857\n",
            "Loss after iteration 64800 is 17031.08614987761\n",
            "Loss after iteration 64900 is 17025.9186857165\n",
            "Loss after iteration 65000 is 17020.757502107146\n",
            "Loss after iteration 65100 is 17015.602586096065\n",
            "Loss after iteration 65200 is 17010.453924812533\n",
            "Loss after iteration 65300 is 17005.311505467515\n",
            "Loss after iteration 65400 is 17000.175315352564\n",
            "Loss after iteration 65500 is 16995.045341838773\n",
            "Loss after iteration 65600 is 16989.921572375715\n",
            "Loss after iteration 65700 is 16984.80399449041\n",
            "Loss after iteration 65800 is 16979.69259578632\n",
            "Loss after iteration 65900 is 16974.587363942344\n",
            "Loss after iteration 66000 is 16969.488286711836\n",
            "Loss after iteration 66100 is 16964.39535192163\n",
            "Loss after iteration 66200 is 16959.30854747109\n",
            "Loss after iteration 66300 is 16954.227861331165\n",
            "Loss after iteration 66400 is 16949.153281543477\n",
            "Loss after iteration 66500 is 16944.08479621938\n",
            "Loss after iteration 66600 is 16939.02239353911\n",
            "Loss after iteration 66700 is 16933.966061750863\n",
            "Loss after iteration 66800 is 16928.91578916995\n",
            "Loss after iteration 66900 is 16923.871564177913\n",
            "Loss after iteration 67000 is 16918.833375221722\n",
            "Loss after iteration 67100 is 16913.80121081289\n",
            "Loss after iteration 67200 is 16908.77505952672\n",
            "Loss after iteration 67300 is 16903.75491000146\n",
            "Loss after iteration 67400 is 16898.740750937504\n",
            "Loss after iteration 67500 is 16893.73257109665\n",
            "Loss after iteration 67600 is 16888.7303593013\n",
            "Loss after iteration 67700 is 16883.734104433708\n",
            "Loss after iteration 67800 is 16878.74379543525\n",
            "Loss after iteration 67900 is 16873.759421305676\n",
            "Loss after iteration 68000 is 16868.780971102384\n",
            "Loss after iteration 68100 is 16863.808433939743\n",
            "Loss after iteration 68200 is 16858.841798988342\n",
            "Loss after iteration 68300 is 16853.881055474343\n",
            "Loss after iteration 68400 is 16848.926192678788\n",
            "Loss after iteration 68500 is 16843.977199936908\n",
            "Loss after iteration 68600 is 16839.034066637505\n",
            "Loss after iteration 68700 is 16834.09678222228\n",
            "Loss after iteration 68800 is 16829.16533618518\n",
            "Loss after iteration 68900 is 16824.239718071818\n",
            "Loss after iteration 69000 is 16819.319917478806\n",
            "Loss after iteration 69100 is 16814.40592405315\n",
            "Loss after iteration 69200 is 16809.497727491682\n",
            "Loss after iteration 69300 is 16804.595317540457\n",
            "Loss after iteration 69400 is 16799.698683994135\n",
            "Loss after iteration 69500 is 16794.80781669545\n",
            "Loss after iteration 69600 is 16789.922705534635\n",
            "Loss after iteration 69700 is 16785.043340448843\n",
            "Loss after iteration 69800 is 16780.169711421644\n",
            "Loss after iteration 69900 is 16775.30180848243\n",
            "Loss after iteration 70000 is 16770.43962170593\n",
            "Loss after iteration 70100 is 16765.583141211668\n",
            "Loss after iteration 70200 is 16760.73235716344\n",
            "Loss after iteration 70300 is 16755.88725976883\n",
            "Loss after iteration 70400 is 16751.04783927867\n",
            "Loss after iteration 70500 is 16746.2140859866\n",
            "Loss after iteration 70600 is 16741.385990228533\n",
            "Loss after iteration 70700 is 16736.56354238221\n",
            "Loss after iteration 70800 is 16731.746732866723\n",
            "Loss after iteration 70900 is 16726.935552142048\n",
            "Loss after iteration 71000 is 16722.12999070857\n",
            "Loss after iteration 71100 is 16717.330039106673\n",
            "Loss after iteration 71200 is 16712.535687916272\n",
            "Loss after iteration 71300 is 16707.746927756358\n",
            "Loss after iteration 71400 is 16702.96374928461\n",
            "Loss after iteration 71500 is 16698.18614319697\n",
            "Loss after iteration 71600 is 16693.414100227157\n",
            "Loss after iteration 71700 is 16688.64761114636\n",
            "Loss after iteration 71800 is 16683.886666762744\n",
            "Loss after iteration 71900 is 16679.1312579211\n",
            "Loss after iteration 72000 is 16674.38137550246\n",
            "Loss after iteration 72100 is 16669.637010423663\n",
            "Loss after iteration 72200 is 16664.898153636997\n",
            "Loss after iteration 72300 is 16660.16479612985\n",
            "Loss after iteration 72400 is 16655.436928924322\n",
            "Loss after iteration 72500 is 16650.71454307682\n",
            "Loss after iteration 72600 is 16645.997629677786\n",
            "Loss after iteration 72700 is 16641.286179851246\n",
            "Loss after iteration 72800 is 16636.580184754523\n",
            "Loss after iteration 72900 is 16631.879635577905\n",
            "Loss after iteration 73000 is 16627.184523544245\n",
            "Loss after iteration 73100 is 16622.494839908686\n",
            "Loss after iteration 73200 is 16617.810575958294\n",
            "Loss after iteration 73300 is 16613.13172301177\n",
            "Loss after iteration 73400 is 16608.458272419095\n",
            "Loss after iteration 73500 is 16603.790215561254\n",
            "Loss after iteration 73600 is 16599.12754384987\n",
            "Loss after iteration 73700 is 16594.470248726975\n",
            "Loss after iteration 73800 is 16589.818321664632\n",
            "Loss after iteration 73900 is 16585.17175416471\n",
            "Loss after iteration 74000 is 16580.530537758532\n",
            "Loss after iteration 74100 is 16575.894664006624\n",
            "Loss after iteration 74200 is 16571.264124498415\n",
            "Loss after iteration 74300 is 16566.638910851987\n",
            "Loss after iteration 74400 is 16562.019014713747\n",
            "Loss after iteration 74500 is 16557.404427758207\n",
            "Loss after iteration 74600 is 16552.795141687675\n",
            "Loss after iteration 74700 is 16548.191148232036\n",
            "Loss after iteration 74800 is 16543.59243914844\n",
            "Loss after iteration 74900 is 16538.99900622111\n",
            "Loss after iteration 75000 is 16534.410841261033\n",
            "Loss after iteration 75100 is 16529.82793610573\n",
            "Loss after iteration 75200 is 16525.250282619032\n",
            "Loss after iteration 75300 is 16520.6778726908\n",
            "Loss after iteration 75400 is 16516.110698236716\n",
            "Loss after iteration 75500 is 16511.548751198054\n",
            "Loss after iteration 75600 is 16506.992023541414\n",
            "Loss after iteration 75700 is 16502.440507258536\n",
            "Loss after iteration 75800 is 16497.89419436604\n",
            "Loss after iteration 75900 is 16493.353076905227\n",
            "Loss after iteration 76000 is 16488.81714694186\n",
            "Loss after iteration 76100 is 16484.28639656592\n",
            "Loss after iteration 76200 is 16479.760817891438\n",
            "Loss after iteration 76300 is 16475.24040305625\n",
            "Loss after iteration 76400 is 16470.72514422182\n",
            "Loss after iteration 76500 is 16466.215033573004\n",
            "Loss after iteration 76600 is 16461.710063317885\n",
            "Loss after iteration 76700 is 16457.21022568755\n",
            "Loss after iteration 76800 is 16452.715512935894\n",
            "Loss after iteration 76900 is 16448.225917339463\n",
            "Loss after iteration 77000 is 16443.74143119721\n",
            "Loss after iteration 77100 is 16439.262046830354\n",
            "Loss after iteration 77200 is 16434.787756582176\n",
            "Loss after iteration 77300 is 16430.318552817836\n",
            "Loss after iteration 77400 is 16425.854427924212\n",
            "Loss after iteration 77500 is 16421.395374309708\n",
            "Loss after iteration 77600 is 16416.941384404054\n",
            "Loss after iteration 77700 is 16412.492450658214\n",
            "Loss after iteration 77800 is 16408.04856554411\n",
            "Loss after iteration 77900 is 16403.609721554563\n",
            "Loss after iteration 78000 is 16399.17591120303\n",
            "Loss after iteration 78100 is 16394.747127023526\n",
            "Loss after iteration 78200 is 16390.3233615704\n",
            "Loss after iteration 78300 is 16385.904607418226\n",
            "Loss after iteration 78400 is 16381.49085716159\n",
            "Loss after iteration 78500 is 16377.082103415014\n",
            "Loss after iteration 78600 is 16372.678338812722\n",
            "Loss after iteration 78700 is 16368.27955600855\n",
            "Loss after iteration 78800 is 16363.885747675786\n",
            "Loss after iteration 78900 is 16359.496906506993\n",
            "Loss after iteration 79000 is 16355.113025213914\n",
            "Loss after iteration 79100 is 16350.734096527296\n",
            "Loss after iteration 79200 is 16346.360113196748\n",
            "Loss after iteration 79300 is 16341.991067990639\n",
            "Loss after iteration 79400 is 16337.62695369593\n",
            "Loss after iteration 79500 is 16333.267763118038\n",
            "Loss after iteration 79600 is 16328.913489080729\n",
            "Loss after iteration 79700 is 16324.564124425968\n",
            "Loss after iteration 79800 is 16320.219662013798\n",
            "Loss after iteration 79900 is 16315.8800947222\n",
            "Loss after iteration 80000 is 16311.545415446994\n",
            "Loss after iteration 80100 is 16307.215617101683\n",
            "Loss after iteration 80200 is 16302.890692617364\n",
            "Loss after iteration 80300 is 16298.570634942585\n",
            "Loss after iteration 80400 is 16294.255437043223\n",
            "Loss after iteration 80500 is 16289.945091902384\n",
            "Loss after iteration 80600 is 16285.639592520285\n",
            "Loss after iteration 80700 is 16281.338931914124\n",
            "Loss after iteration 80800 is 16277.04310311798\n",
            "Loss after iteration 80900 is 16272.7520991827\n",
            "Loss after iteration 81000 is 16268.465913175794\n",
            "Loss after iteration 81100 is 16264.184538181305\n",
            "Loss after iteration 81200 is 16259.907967299725\n",
            "Loss after iteration 81300 is 16255.636193647888\n",
            "Loss after iteration 81400 is 16251.369210358853\n",
            "Loss after iteration 81500 is 16247.107010581807\n",
            "Loss after iteration 81600 is 16242.849587481976\n",
            "Loss after iteration 81700 is 16238.596934240486\n",
            "Loss after iteration 81800 is 16234.349044054323\n",
            "Loss after iteration 81900 is 16230.10591013618\n",
            "Loss after iteration 82000 is 16225.867525714388\n",
            "Loss after iteration 82100 is 16221.63388403282\n",
            "Loss after iteration 82200 is 16217.404978350787\n",
            "Loss after iteration 82300 is 16213.180801942957\n",
            "Loss after iteration 82400 is 16208.961348099252\n",
            "Loss after iteration 82500 is 16204.746610124765\n",
            "Loss after iteration 82600 is 16200.53658133966\n",
            "Loss after iteration 82700 is 16196.331255079105\n",
            "Loss after iteration 82800 is 16192.130624693154\n",
            "Loss after iteration 82900 is 16187.934683546686\n",
            "Loss after iteration 83000 is 16183.743425019313\n",
            "Loss after iteration 83100 is 16179.556842505297\n",
            "Loss after iteration 83200 is 16175.374929413443\n",
            "Loss after iteration 83300 is 16171.197679167068\n",
            "Loss after iteration 83400 is 16167.025085203866\n",
            "Loss after iteration 83500 is 16162.857140975862\n",
            "Loss after iteration 83600 is 16158.693839949327\n",
            "Loss after iteration 83700 is 16154.53517560467\n",
            "Loss after iteration 83800 is 16150.381141436417\n",
            "Loss after iteration 83900 is 16146.231730953092\n",
            "Loss after iteration 84000 is 16142.086937677142\n",
            "Loss after iteration 84100 is 16137.946755144885\n",
            "Loss after iteration 84200 is 16133.811176906423\n",
            "Loss after iteration 84300 is 16129.68019652558\n",
            "Loss after iteration 84400 is 16125.553807579785\n",
            "Loss after iteration 84500 is 16121.432003660078\n",
            "Loss after iteration 84600 is 16117.314778370983\n",
            "Loss after iteration 84700 is 16113.202125330448\n",
            "Loss after iteration 84800 is 16109.094038169782\n",
            "Loss after iteration 84900 is 16104.990510533607\n",
            "Loss after iteration 85000 is 16100.891536079747\n",
            "Loss after iteration 85100 is 16096.797108479197\n",
            "Loss after iteration 85200 is 16092.707221416042\n",
            "Loss after iteration 85300 is 16088.621868587405\n",
            "Loss after iteration 85400 is 16084.54104370337\n",
            "Loss after iteration 85500 is 16080.464740486921\n",
            "Loss after iteration 85600 is 16076.39295267389\n",
            "Loss after iteration 85700 is 16072.32567401287\n",
            "Loss after iteration 85800 is 16068.262898265191\n",
            "Loss after iteration 85900 is 16064.204619204827\n",
            "Loss after iteration 86000 is 16060.150830618342\n",
            "Loss after iteration 86100 is 16056.101526304854\n",
            "Loss after iteration 86200 is 16052.05670007594\n",
            "Loss after iteration 86300 is 16048.016345755614\n",
            "Loss after iteration 86400 is 16043.980457180245\n",
            "Loss after iteration 86500 is 16039.949028198514\n",
            "Loss after iteration 86600 is 16035.922052671347\n",
            "Loss after iteration 86700 is 16031.899524471864\n",
            "Loss after iteration 86800 is 16027.881437485328\n",
            "Loss after iteration 86900 is 16023.867785609087\n",
            "Loss after iteration 87000 is 16019.858562752524\n",
            "Loss after iteration 87100 is 16015.853762837\n",
            "Loss after iteration 87200 is 16011.853379795797\n",
            "Loss after iteration 87300 is 16007.857407574083\n",
            "Loss after iteration 87400 is 16003.86584012884\n",
            "Loss after iteration 87500 is 15999.878671428818\n",
            "Loss after iteration 87600 is 15995.895895454505\n",
            "Loss after iteration 87700 is 15991.917506198051\n",
            "Loss after iteration 87800 is 15987.943497663222\n",
            "Loss after iteration 87900 is 15983.973863865373\n",
            "Loss after iteration 88000 is 15980.008598831371\n",
            "Loss after iteration 88100 is 15976.047696599573\n",
            "Loss after iteration 88200 is 15972.091151219765\n",
            "Loss after iteration 88300 is 15968.138956753106\n",
            "Loss after iteration 88400 is 15964.191107272094\n",
            "Loss after iteration 88500 is 15960.247596860534\n",
            "Loss after iteration 88600 is 15956.308419613462\n",
            "Loss after iteration 88700 is 15952.373569637117\n",
            "Loss after iteration 88800 is 15948.443041048895\n",
            "Loss after iteration 88900 is 15944.51682797731\n",
            "Loss after iteration 89000 is 15940.594924561943\n",
            "Loss after iteration 89100 is 15936.677324953387\n",
            "Loss after iteration 89200 is 15932.764023313237\n",
            "Loss after iteration 89300 is 15928.855013814016\n",
            "Loss after iteration 89400 is 15924.950290639143\n",
            "Loss after iteration 89500 is 15921.049847982904\n",
            "Loss after iteration 89600 is 15917.153680050389\n",
            "Loss after iteration 89700 is 15913.261781057472\n",
            "Loss after iteration 89800 is 15909.374145230759\n",
            "Loss after iteration 89900 is 15905.490766807541\n",
            "Loss after iteration 90000 is 15901.611640035771\n",
            "Loss after iteration 90100 is 15897.73675917401\n",
            "Loss after iteration 90200 is 15893.86611849141\n",
            "Loss after iteration 90300 is 15889.999712267638\n",
            "Loss after iteration 90400 is 15886.13753479288\n",
            "Loss after iteration 90500 is 15882.279580367778\n",
            "Loss after iteration 90600 is 15878.425843303383\n",
            "Loss after iteration 90700 is 15874.576317921157\n",
            "Loss after iteration 90800 is 15870.730998552883\n",
            "Loss after iteration 90900 is 15866.889879540688\n",
            "Loss after iteration 91000 is 15863.05295523695\n",
            "Loss after iteration 91100 is 15859.22022000429\n",
            "Loss after iteration 91200 is 15855.391668215561\n",
            "Loss after iteration 91300 is 15851.56729425375\n",
            "Loss after iteration 91400 is 15847.747092511998\n",
            "Loss after iteration 91500 is 15843.931057393544\n",
            "Loss after iteration 91600 is 15840.119183311688\n",
            "Loss after iteration 91700 is 15836.311464689772\n",
            "Loss after iteration 91800 is 15832.507895961131\n",
            "Loss after iteration 91900 is 15828.708471569054\n",
            "Loss after iteration 92000 is 15824.913185966781\n",
            "Loss after iteration 92100 is 15821.122033617436\n",
            "Loss after iteration 92200 is 15817.335008994009\n",
            "Loss after iteration 92300 is 15813.552106579324\n",
            "Loss after iteration 92400 is 15809.773320866017\n",
            "Loss after iteration 92500 is 15805.998646356484\n",
            "Loss after iteration 92600 is 15802.22807756285\n",
            "Loss after iteration 92700 is 15798.461609006963\n",
            "Loss after iteration 92800 is 15794.699235220342\n",
            "Loss after iteration 92900 is 15790.940950744143\n",
            "Loss after iteration 93000 is 15787.186750129138\n",
            "Loss after iteration 93100 is 15783.436627935685\n",
            "Loss after iteration 93200 is 15779.690578733702\n",
            "Loss after iteration 93300 is 15775.948597102619\n",
            "Loss after iteration 93400 is 15772.210677631358\n",
            "Loss after iteration 93500 is 15768.476814918324\n",
            "Loss after iteration 93600 is 15764.747003571341\n",
            "Loss after iteration 93700 is 15761.021238207637\n",
            "Loss after iteration 93800 is 15757.299513453829\n",
            "Loss after iteration 93900 is 15753.581823945873\n",
            "Loss after iteration 94000 is 15749.868164329057\n",
            "Loss after iteration 94100 is 15746.158529257951\n",
            "Loss after iteration 94200 is 15742.452913396397\n",
            "Loss after iteration 94300 is 15738.751311417469\n",
            "Loss after iteration 94400 is 15735.05371800345\n",
            "Loss after iteration 94500 is 15731.360127845808\n",
            "Loss after iteration 94600 is 15727.670535645166\n",
            "Loss after iteration 94700 is 15723.984936111277\n",
            "Loss after iteration 94800 is 15720.303323962991\n",
            "Loss after iteration 94900 is 15716.625693928232\n",
            "Loss after iteration 95000 is 15712.952040743987\n",
            "Loss after iteration 95100 is 15709.282359156248\n",
            "Loss after iteration 95200 is 15705.616643920019\n",
            "Loss after iteration 95300 is 15701.954889799257\n",
            "Loss after iteration 95400 is 15698.297091566883\n",
            "Loss after iteration 95500 is 15694.643244004721\n",
            "Loss after iteration 95600 is 15690.993341903511\n",
            "Loss after iteration 95700 is 15687.347380062833\n",
            "Loss after iteration 95800 is 15683.705353291149\n",
            "Loss after iteration 95900 is 15680.067256405706\n",
            "Loss after iteration 96000 is 15676.433084232569\n",
            "Loss after iteration 96100 is 15672.802831606563\n",
            "Loss after iteration 96200 is 15669.176493371273\n",
            "Loss after iteration 96300 is 15665.554064378995\n",
            "Loss after iteration 96400 is 15661.935539490725\n",
            "Loss after iteration 96500 is 15658.320913576139\n",
            "Loss after iteration 96600 is 15654.71018151356\n",
            "Loss after iteration 96700 is 15651.103338189943\n",
            "Loss after iteration 96800 is 15647.500378500863\n",
            "Loss after iteration 96900 is 15643.901297350438\n",
            "Loss after iteration 97000 is 15640.306089651378\n",
            "Loss after iteration 97100 is 15636.714750324923\n",
            "Loss after iteration 97200 is 15633.127274300812\n",
            "Loss after iteration 97300 is 15629.543656517293\n",
            "Loss after iteration 97400 is 15625.96389192108\n",
            "Loss after iteration 97500 is 15622.387975467314\n",
            "Loss after iteration 97600 is 15618.815902119584\n",
            "Loss after iteration 97700 is 15615.24766684987\n",
            "Loss after iteration 97800 is 15611.683264638523\n",
            "Loss after iteration 97900 is 15608.122690474276\n",
            "Loss after iteration 98000 is 15604.565939354172\n",
            "Loss after iteration 98100 is 15601.01300628359\n",
            "Loss after iteration 98200 is 15597.463886276188\n",
            "Loss after iteration 98300 is 15593.918574353906\n",
            "Loss after iteration 98400 is 15590.377065546925\n",
            "Loss after iteration 98500 is 15586.839354893673\n",
            "Loss after iteration 98600 is 15583.305437440777\n",
            "Loss after iteration 98700 is 15579.775308243048\n",
            "Loss after iteration 98800 is 15576.24896236347\n",
            "Loss after iteration 98900 is 15572.726394873183\n",
            "Loss after iteration 99000 is 15569.207600851438\n",
            "Loss after iteration 99100 is 15565.692575385609\n",
            "Loss after iteration 99200 is 15562.181313571147\n",
            "Loss after iteration 99300 is 15558.673810511567\n",
            "Loss after iteration 99400 is 15555.170061318438\n",
            "Loss after iteration 99500 is 15551.670061111356\n",
            "Loss after iteration 99600 is 15548.173805017912\n",
            "Loss after iteration 99700 is 15544.681288173704\n",
            "Loss after iteration 99800 is 15541.192505722283\n",
            "Loss after iteration 99900 is 15537.707452815148\n",
            "Train Error:  15534.260919472361 , Test Error:  5046.733372349463\n"
          ]
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "\"\"\"\n",
        "Manually choose the hyperparameters (learning rate and number of features) and train the model.\n",
        "Then compare the performance with random chosen hyperparameters and manually chosen hyperparameters.\n",
        "\"\"\"\n",
        "### START CODE HERE ###\n",
        "# manually chosen hyperparameters\n",
        "alpha_manual = 0.00001\n",
        "num_iters = 100000\n",
        "X_manual = X[:, :13]\n",
        "X_manual = min_max_scaler(X_manual)\n",
        "X_train_manual, X_test_manual, y_train_manual, y_test_manual = train_test_split(X_manual, y, test_size=0.25, random_state=42)\n",
        "initial_w_manual, initial_b_manual, alpha_manual = initialize_parameters()\n",
        "w_manual,b_manual,loss_hist_manual = batch_gradient_descent(X_train_manual ,y_train_manual, initial_w_manual, initial_b_manual, alpha_manual, num_iters)\n",
        "train_error_manual = loss_function(X_train_manual,y_train_manual,w_manual,b_manual)\n",
        "test_error_manual = loss_function(X_test_manual,y_test_manual,w_manual,b_manual)\n",
        "print(\"Train Error: \",train_error_manual, \", Test Error: \",test_error_manual)\n",
        "\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ0CviUnZui0"
      },
      "source": [
        "## Part 2: Logistic Regression\n",
        "\n",
        "### Problem Statement\n",
        "A Food grain industry requires an efficient classification system to help in sorting food grain species. You have to develop a Logistic Regression model for this purpose.\n",
        " Given various features of a rice grain such as area, perimeter, axis lengths etc. as input features, the task is to build a logistic regression model to predict the species of the food grain.\n",
        "\n",
        "### Dataset Description\n",
        "\n",
        "**For Even Roll Number Students**\n",
        "\n",
        "Dataset Filename: Rice_Classification.csv\n",
        "\n",
        "Attribute Information:\n",
        "+ Area: Returns the number of pixels within the boundaries of the rice grain.\n",
        "+ Perimeter: Calculates the circumference by calculating the distance between pixels around the boundaries of the rice grain.\n",
        "+ Major Axis Length: The longest line that can be drawn on the rice grain, i.e. the main axis distance, gives.\n",
        "+ Minor Axis Length: The shortest line that can be drawn on the rice grain, i.e. the small axis distance, gives.\n",
        "+ Eccentricity: It measures how round the ellipse, which has the same moments as the rice grain, is.\n",
        "+ Convex Area: Returns the pixel count of the smallest convex shell of the region formed by the rice grain.\n",
        "+ Extent: Returns the ratio of the region formed by the rice grain to the bounding box pixels.\n",
        "\n",
        "Target Variable: Class: Cammeo and Osmancik\n",
        "\n",
        "**For Odd Roll Number Students**\n",
        "\n",
        "Dataset Filename: Pumpkin_Seeds_Dataset.csv\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "+ Area: Represents the number of pixels within the boundaries of the pumpkin seed.\n",
        "\n",
        "+ Perimeter: Calculates the circumference by measuring the distance between pixels around the boundary of the pumpkin seed.\n",
        "\n",
        "+ Major Axis Length: The longest line that can be drawn on the pumpkin seed, representing the main axis distance.\n",
        "\n",
        "+ Minor Axis Length: The shortest line that can be drawn on the pumpkin seed, representing the minor axis distance.\n",
        "\n",
        "+ Convex Area: Returns the pixel count of the smallest convex shell that can contain the pumpkin seed.\n",
        "\n",
        "+ Equivalent Diameter: Diameter of a circle with the same area as the pumpkin seed.\n",
        "\n",
        "+ Eccentricity: This measures how round the ellipse, which has the same moments as the pumpkin seed has.\n",
        "\n",
        "+ Solidity: This is the ratio of the area of the pumpkin seed to the area of its convex hull. It measures the extent to which the shape is convex.\n",
        "\n",
        "+ Extent: Returns the ratio of the area of the pumpkin seed to the area of its bounding box.\n",
        "\n",
        "+ Roundness: Measure of how closely the shape of the pumpkin seed approaches that of a circle.\n",
        "\n",
        "+ Aspect Ratio: Ratio of the major axis length to the minor axis length.\n",
        "\n",
        "+ Compactness: Measure of the shape's compactness, which is the shape's deviation from being a perfect circle. In essence, compactness quantifies how efficiently an object's area is packed within its perimeter.\n",
        "\n",
        "Target Variable: Class: Çerçevelik and Ürgüp Sivrisi\n",
        "\n",
        "\n",
        "\n",
        "These are the following steps or functions that you have to complete to create and train the linear regression model:\n",
        "1. Reading the data\n",
        "2. Creating the sigmoid function\n",
        "2. Computing the loss function\n",
        "3. Computing the gradient of the loss\n",
        "4. Training the model using Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "T27XOBZrRZ4M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import logsumexp\n",
        "import copy\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3luUk4L0WO_"
      },
      "source": [
        "### 2.1. Reading the data\n",
        "\n",
        "In the function ```load_data```, you have to read data from the file, store it in a dataframe and split the data from the dataframe into two numpy arrays X and y.\n",
        "\n",
        "**X** : data of the input features\n",
        "\n",
        "**y**  : data of the class labels\n",
        "\n",
        "The class labels in **y** should be replaced with '0' and '1', for corresponding classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7-RV0JgEZi1h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 1 1 1]\n",
            "Shape of X:  (2500, 12) Shape of y:  (2500,)\n"
          ]
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def load_data(filepath):\n",
        "    \"\"\"\n",
        "    This function loads the data into a pandas dataframe and converts it into X and y numpy arrays\n",
        "    y should be a binary numpy array with values 0 and 1, for 2 different classes\n",
        "    Args:\n",
        "        filepath: File path as a string\n",
        "    Returns:\n",
        "        X: Input data of the shape (# of samples, # of input features)\n",
        "        y: Target variable of the shape (# of sample,) with values 0 and 1, for 2 different classes\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    data = pd.read_csv(filepath)\n",
        "    target_column = data.columns[-1]\n",
        "    \n",
        "    X = data.drop(columns=[target_column]).values\n",
        "    y = data[target_column].values\n",
        "    # convert entries of seed names in y to 0 or 1\n",
        "    y = np.where(y==y[0], 0, 1)\n",
        "    print(y)    \n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X,y\n",
        "\n",
        "### START CODE HERE ###\n",
        "## set the file path\n",
        "filepath = 'Pumpkin_Seeds_Dataset.csv'\n",
        "### END CODE HERE ###\n",
        "\n",
        "X, y = load_data(filepath)\n",
        "\n",
        "print(\"Shape of X: \",X.shape, \"Shape of y: \",y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLGPG1YU-C47"
      },
      "source": [
        "We will not use all the features from X.\n",
        "\n",
        "For Even Roll Number Students: Set the last two digits of your roll number as the random seed and pick a number r between 4 and 7 (both inclusive) randomly. Use the first r features of the numpy array X.\n",
        "\n",
        "For Odd Roll Number Students: Set the last two digits of your roll number as the random seed and pick a number r between 8 and 12 (both inclusive) randomly. Use the first r features of the numpy array X."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JNsDb2Ae94s6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n",
            "Shape of X:  (2500, 9) Shape of y:  (2500,)\n"
          ]
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def random_feature_selection(X):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        X: Input data of the shape (# of samples, # of input features)\n",
        "    Returns:\n",
        "        X_new: New input data of the shape (# of samples, r) containg only the first r features from X\n",
        "    \"\"\"\n",
        "\n",
        "    X_new = None\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    np.random.seed(39)\n",
        "    r = np.random.randint(8, 12)\n",
        "    print(r)\n",
        "    X_new = X[:, :r]\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X_new\n",
        "\n",
        "X = random_feature_selection(X)\n",
        "print(\"Shape of X: \",X.shape, \"Shape of y: \",y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4u-qBe34wC9"
      },
      "source": [
        "We need to pre-process the data. We are using min-max scaler to scale the input data ($X$).\n",
        "\n",
        "After that, we split the data (```X``` and ```y```) into a training dataset (```X_train``` and ```y_train```) and test dataset (```X_test``` and ```y_test```)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YTO4etePa0i1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train:  (1875, 9) Shape of y_train:  (1875,)\n",
            "Shape of X_test:  (625, 9) Shape of y_test:  (625,)\n"
          ]
        }
      ],
      "source": [
        "## Data scaling and train-test split\n",
        "\n",
        "def train_test_split(X, y, test_size=0.25, random_state=None):\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    split_index = int(X.shape[0] * (1 - test_size))\n",
        "\n",
        "    train_indices = indices[:split_index]\n",
        "    test_indices = indices[split_index:]\n",
        "\n",
        "    X_train = X[train_indices]\n",
        "    X_test = X[test_indices]\n",
        "    y_train = y[train_indices]\n",
        "    y_test = y[test_indices]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def min_max_scaler(X, feature_range=(0, 1)):\n",
        "    X_min = np.min(X, axis=0)\n",
        "    X_max = np.max(X, axis=0)\n",
        "\n",
        "    X_scaled = (X-X_min)/(X_max-X_min)\n",
        "\n",
        "    return X_scaled\n",
        "\n",
        "# Feature normalization\n",
        "X = min_max_scaler(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "print(\"Shape of X_train: \",X_train.shape, \"Shape of y_train: \",y_train.shape)\n",
        "print(\"Shape of X_test: \",X_test.shape, \"Shape of y_test: \",y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8SbmjD945Ra"
      },
      "source": [
        "### 2.2. Creating the Sigmoid Function\n",
        "Recall that for logistic regression, the model is represented as\n",
        "\n",
        "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
        "where function $g$ is the sigmoid function. The sigmoid function is defined as:\n",
        "\n",
        "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "\n",
        "The function below returns the value of the sigmoid function for an input numpy array z. If the numpy array 'z' stores multiple numbers, we'd like to apply the sigmoid function to each value in the input array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "G44B1vITbBpE"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Args:\n",
        "        z: A scalar or numpy array of any size.\n",
        "\n",
        "    Returns:\n",
        "        g: sigmoid(z)\n",
        "    \"\"\"\n",
        "    g = None\n",
        "    z = z.astype(float)\n",
        "    ### START CODE HERE ###\n",
        "    g = 1/(1 + np.exp(-z))\n",
        "\n",
        "\n",
        "    ### END SOLUTION ###\n",
        "\n",
        "    return g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV-Ygf1R8HMl"
      },
      "source": [
        "### 2.3. Computing the loss Function\n",
        "\n",
        "Recall that for logistic regression, the cost function is of the form\n",
        "\n",
        "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
        "\n",
        "where\n",
        "* m is the number of training examples in the dataset\n",
        "\n",
        "\n",
        "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is -\n",
        "\n",
        "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
        "    \n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label\n",
        "\n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function.\n",
        "\n",
        "Please complete the function loss_function that takes\n",
        "\n",
        " **X**  (input features)\n",
        "\n",
        " **y**  (class labels)\n",
        "\n",
        " **w**  (Parameters of the logistic regression model, (excluding the bias), a numpy array of the shape(1, number of features))\n",
        "\n",
        " **b**  (Bias value of the logistic regression model)\n",
        "\n",
        " You can use the Sigmoid function that you implemented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "h9RVguhHbEVk"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "\n",
        "def loss_function(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the loss function for all the training examples\n",
        "    Args:\n",
        "          X: Input data of the shape (# of training samples, # of input features)\n",
        "          y: Target variable of the shape (# of training sample,)\n",
        "          w: Parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
        "          b: Bias parameter (scalar) of the logistic regression model\n",
        "\n",
        "    Returns:\n",
        "          total_cost: The loss function value of using w and b as the parameters to fit the data points in X and y\n",
        "\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "\n",
        "    total_cost = 0\n",
        "    ### START CODE HERE ###\n",
        "    z = (np.dot(X, w.T) + b).flatten()\n",
        "    A = sigmoid(z)\n",
        "\n",
        "    total_cost = -np.sum(y * np.log(A) + (1 - y) * np.log(1 - A))/m\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return total_cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9YmUPnsASYE"
      },
      "source": [
        "### 2.4. Computing the Gradient of the Loss\n",
        "\n",
        "Recall that the gradient descent algorithm is:\n",
        "\n",
        "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
        "\n",
        "where, parameters $b$, $w_j$ are all updated simultaniously\n",
        "\n",
        "In this step, you are required to complete the `compute_gradient_logistic_regression` function to compute $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ from equations (2) and (3) below.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
        "$$\n",
        "* m is the number of training examples in the dataset\n",
        "\n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the actual label\n",
        "\n",
        "You can use the sigmoid function that you implemented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PWxbewKkbIB8"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def compute_gradient_logistic_regression(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient values of the loss function\n",
        "    Args:\n",
        "       X: Input data of the shape (# of training samples, # of input features)\n",
        "       y: Target variable of the shape (# of training sample,)\n",
        "       w: Parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
        "       b: Bias parameter of the logistic regression model of the shape (1,1) or a scaler\n",
        "    Returns:\n",
        "       dL_dw : The gradient of the cost w.r.t. the parameters w with shape same as w\n",
        "       dL_db : The gradient of the cost w.r.t. the parameter b with shape same as b\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros(w.shape)\n",
        "    dj_db = 0\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    \n",
        "   #  dj_db = np.sum((sigmoid(np.dot(X, w.T) + b)).flatten() - y)/m\n",
        "   #  dj_dw = np.dot((sigmoid(np.dot(X, w.T) + b).flatten() - y).T, X).reshape(1, -1)/m\n",
        "    z = (np.dot(X, w.T) + b).flatten()\n",
        "    A = sigmoid(z)\n",
        "    dj_db = np.sum(A - y)/m\n",
        "    dj_dw = np.dot((A - y).T, X)/m\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "\n",
        "    return dj_db, dj_dw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZMYX655MRFX"
      },
      "source": [
        "### 2.5. Training the model using Batch Gradient Descent\n",
        "\n",
        "Please complete the batch gradient descent algorithm for logistic regression to train and learn the parameters of the logistic regression model. You have to use ```loss_function``` and ```compute_gradient_logistic regression``` functions that you have implemented earlier in this assignment.\n",
        "\n",
        "In this ```batch_gradient_descent_logistic_regression``` function, you have to compute the gradient for the training samples and update the parameters $w$ and $b$ in every iteration:\n",
        "\n",
        "+ $w \\leftarrow w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
        "\n",
        "+ $b \\leftarrow b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
        "\n",
        "This function takes   ```X```    (input features),  ```y```  (class labels),  ```w_in```  (intial values of parameters(excluding bias)),  ```b_in```  (initial value for bias),  ```num_iters```   (number of iterations of training) as input.\n",
        "\n",
        "Additionally, you have compute the loss function values in every iteration and store it in the list variable ```loss_hist``` and print the loss value after every 100 iterations during the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JOi07RVNbL_7"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def batch_gradient_descent_logistic_regression(X, y, w_in, b_in, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Batch gradient descent to learn the parameters (w and b) of the linear regression model and to print loss values\n",
        "    every 100 iterations\n",
        "\n",
        "    Args:\n",
        "        X: Input data of the shape (# of training samples, # of input features)\n",
        "        y: Target variable of the shape (# of training sample,)\n",
        "        w_in: Initial parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
        "        b_in: Initial bias parameter (scalar) of the logistic regression model\n",
        "        alpha: Learning rate\n",
        "        num_iters: number of iterations\n",
        "    Returns\n",
        "        w: Updated values of parameters of the model after training\n",
        "        b: Updated bias of the model after training\n",
        "        loss_hist: List of loss values for every iteration\n",
        "    \"\"\"\n",
        "\n",
        "    # number of training examples\n",
        "    m = len(X)\n",
        "\n",
        "    # list to store the loss values for every iterations\n",
        "    loss_hist = []\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "        dj_db, dj_dw = compute_gradient_logistic_regression(X, y, w_in, b_in)\n",
        "        w_in = w_in - alpha*dj_dw\n",
        "        b_in = b_in - alpha*dj_db\n",
        "        loss = loss_function(X, y, w_in, b_in)\n",
        "        loss_hist.append(loss)\n",
        "        if i%100 == 0:\n",
        "            print(f\"Loss after iteration {i} is {loss}\")\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return w_in, b_in, loss_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yeLdzT9-5Wc"
      },
      "source": [
        "Now you have to intialize the model parameters ($w$ and $b$) and learning rate (```alpha```). The learning rate ```alpha``` has to be randomly initialized between 0.01 and 0.09. To randomly initialize the learning rate, you have to first set the last two digits of your roll number as the random seed using ```random.seed()``` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MIBltyJh-Tjk"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "## set the last two digits of your roll number as the random seed\n",
        "random_seed = None\n",
        "### START CODE HERE ###\n",
        "random_seed = 39\n",
        "### END CODE HERE ###\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "def initialize_parameters():\n",
        "    \"\"\"\n",
        "    This function randomly initializes the model parameters (w and b) and the hyperparameter alpha\n",
        "    Initial w and b should be randomly sampled from a normal distribution with mean 0\n",
        "    alpha should be randomly initialized between 0.01 and 0.09 by using last two digits of your roll number as the random seed\n",
        "    Args:\n",
        "        None\n",
        "    Returns:\n",
        "        initial_w: Initial parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "        initial_b: Initial bias parameter (scalar) of the linear regression model\n",
        "        alpha: Learning rate\n",
        "    \"\"\"\n",
        "\n",
        "    initial_w = None\n",
        "    initial_b = None\n",
        "    alpha = None\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    np.random.seed(random_seed)\n",
        "    alpha = np.random.uniform(0.01, 0.09)\n",
        "    initial_w = np.random.randn(1, X_train.shape[1])\n",
        "    initial_b = np.random.randn()\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return initial_w,initial_b,alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT5CbQ8dZS_J"
      },
      "source": [
        "The following cell runs the batch gradient algorithm for\n",
        "```num_iterations=1000``` to train the logistic regression model. You can change the number of iterations to check any improvements in the performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KFTZi7pkbOdp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after iteration 0 is 0.9117938555466198\n",
            "Loss after iteration 100 is 0.5993835915634428\n",
            "Loss after iteration 200 is 0.5794186495653917\n",
            "Loss after iteration 300 is 0.562250751527203\n",
            "Loss after iteration 400 is 0.5473604384897955\n",
            "Loss after iteration 500 is 0.5343374584027288\n",
            "Loss after iteration 600 is 0.5228542016109947\n",
            "Loss after iteration 700 is 0.5126497637600546\n",
            "Loss after iteration 800 is 0.5035160828240848\n",
            "Loss after iteration 900 is 0.49528654681414797\n",
            "optimized parameter values w: [[ 0.72363536  1.25322965  1.55397231 -2.40831518 -2.36941369  1.91277455\n",
            "   2.69217641 -1.15897701 -0.10742488]]\n",
            "optimized parameter value b: -1.2269330852593474\n"
          ]
        }
      ],
      "source": [
        "# initialize the parameters (w an b) randomly\n",
        "initial_w, initial_b, alpha = initialize_parameters()\n",
        "num_iterations = 1000\n",
        "\n",
        "w, b, loss_hist = batch_gradient_descent_logistic_regression(X_train ,y_train, initial_w, initial_b, alpha, num_iterations)\n",
        "print(\"optimized parameter values w:\", w)\n",
        "print(\"optimized parameter value b:\", b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYP0-YUyY50x"
      },
      "source": [
        "### 2.6. Final Train and Test Accuracy\n",
        "After the logistic regression model is trained, we will predict the class labels for the training set and test set and we will compute the accuracy.\n",
        "\n",
        "Please complete the `predict` function to produce `1` or `0` predictions given a dataset and a learned parameter vector $w$ and $b$.\n",
        "- First you need to compute the prediction from the model $f(x^{(i)}) = g(w \\cdot x^{(i)})$ for every example\n",
        "\n",
        "- We interpret the output of the model ($f(x^{(i)})$) as the probability that $y^{(i)}=1$ given $x^{(i)}$ and parameterized by $w$.\n",
        "- Therefore, to get a final prediction ($y^{(i)}=0$ or $y^{(i)}=1$) from the logistic regression model, you can use the following heuristic -\n",
        "\n",
        "  if $f(x^{(i)}) >= 0.5$, predict $y^{(i)}=1$\n",
        "  \n",
        "  if $f(x^{(i)}) < 0.5$, predict $y^{(i)}=0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mZDUV2iwbQ_R"
      },
      "outputs": [],
      "source": [
        "def predict(X, w, b):\n",
        "      \"\"\"\n",
        "      Predict whether the label is 0 or 1 using learned logistic regression parameters (w,b)\n",
        "\n",
        "      Args:\n",
        "        X: Input data of shape (number of sample, number of features)\n",
        "        w: Parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
        "        b: Bias parameter of the logistic regression model\n",
        "\n",
        "      Returns:\n",
        "        p: Predictions for X using a threshold at 0.5\n",
        "      \"\"\"\n",
        "      m, n = X.shape\n",
        "      p = np.zeros(m)\n",
        "      ### START CODE HERE###\n",
        "      z = (np.dot(X, w.T) + b).flatten()  \n",
        "      A = sigmoid(z)\n",
        "      p = np.where(A >= 0.5, 1, 0)\n",
        "\n",
        "      ### END CODE HERE ###\n",
        "      return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsrATkIy2KVy"
      },
      "source": [
        "Now let's use this to compute the accuracy on the training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UMmh6WdFbU9Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 85.226667\n",
            "Test Accuracy: 86.240000\n"
          ]
        }
      ],
      "source": [
        "p_train = predict(X_train, w,b)\n",
        "print('Train Accuracy: %f'%(np.mean(p_train == y_train) * 100))\n",
        "p_test = predict(X_test, w,b)\n",
        "print('Test Accuracy: %f'%(np.mean(p_test == y_test) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0VqjFy9ZK7D"
      },
      "source": [
        "Now, we plot the loss function values for every iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fkmP3XHMTW74"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABETklEQVR4nO3deXiU9b3//9dMlsm+byQMhH0NYY+AVVupVD3WpQtVK4it/mxFRU6PQt31ILYe+eFWsT1au9i6V3usQjGKK7KDsu8kQPaQnWwz9/ePSQbGhDHRmbmTyfNxXXOR3EvmPR+FvK7PdlsMwzAEAAAQJKxmFwAAAOBLhBsAABBUCDcAACCoEG4AAEBQIdwAAICgQrgBAABBhXADAACCSqjZBQSa0+nU8ePHFRsbK4vFYnY5AACgCwzDUG1trTIzM2W1eu+b6XPh5vjx47Lb7WaXAQAAvobCwkL179/f6zV9LtzExsZKcjVOXFycydUAAICuqKmpkd1ud/8e96bPhZv2oai4uDjCDQAAvUxXppQwoRgAAAQVwg0AAAgqhBsAABBUCDcAACCoEG4AAEBQIdwAAICgQrgBAABBhXADAACCCuEGAAAEFcINAAAIKoQbAAAQVAg3AAAgqPS5B2f6S3OrU+V1TTIkZSVEml0OAAB9Fj03PrLtaJWmP/yefvq/68wuBQCAPo1w4yPhIa6mbG51mlwJAAB9G+HGR8JDXU3ZRLgBAMBUhBsfCXP33DhMrgQAgL6NcOMjtraemxaHYXIlAAD0bYQbH2kflmp2MCwFAICZCDc+0j6h2OE05HDSewMAgFlMDzdPPfWUsrOzFRERoby8PK1fv/6M17a0tOiBBx7QkCFDFBERodzcXK1cuTKA1Z5ZWOippmTFFAAA5jE13Lz00ktauHCh7r33Xm3evFm5ubmaNWuWSktLO73+rrvu0jPPPKMnnnhCO3fu1I033qjLL79cW7ZsCXDlHbX33EgMTQEAYCaLYRimjaHk5eVpypQpevLJJyVJTqdTdrtdN998sxYtWtTh+szMTN1555266aab3Md+8IMfKDIyUn/96187fY+mpiY1NTW5v6+pqZHdbld1dbXi4uJ89lkMw9CgxW9LkjbcOVOpsTaf/WwAAPq6mpoaxcfHd+n3t2k9N83Nzdq0aZNmzpx5qhirVTNnztTatWs7vaepqUkREREexyIjI/Xxxx+f8X2WLl2q+Ph498tut/vmA3yJxWJhUjEAAD2AaeGmvLxcDodD6enpHsfT09NVXFzc6T2zZs3SsmXLtG/fPjmdTq1evVqvv/66ioqKzvg+ixcvVnV1tftVWFjo089xOnYpBgDAfKZPKO6Oxx57TMOGDdPIkSMVHh6u+fPna968ebJaz/wxbDab4uLiPF7+Eu7e64ZwAwCAWUwLNykpKQoJCVFJSYnH8ZKSEmVkZHR6T2pqqt544w3V19fryJEj2r17t2JiYjR48OBAlPyV6LkBAMB8poWb8PBwTZo0Sfn5+e5jTqdT+fn5mjZtmtd7IyIilJWVpdbWVr322mu69NJL/V1ul/B8KQAAzBdq5psvXLhQc+fO1eTJkzV16lQtX75c9fX1mjdvniRpzpw5ysrK0tKlSyVJ69at07FjxzR+/HgdO3ZM9913n5xOp26//XYzP4ZbWIhFEj03AACYydRwM3v2bJWVlemee+5RcXGxxo8fr5UrV7onGRcUFHjMp2lsbNRdd92lgwcPKiYmRhdddJH+8pe/KCEhwaRP4Ck8NEQSc24AADCTqfvcmKE76+S769KnPtG2wir975zJmjk6/atvAAAAXdIr9rkJRrYQ9rkBAMBshBsfCgtlzg0AAGYj3PhQOD03AACYjnDjQ+7HL9BzAwCAaQg3PtS+WopwAwCAeQg3PsSwFAAA5iPc+FB424TiFnpuAAAwDeHGh+i5AQDAfIQbH2JCMQAA5iPc+BAPzgQAwHyEGx8KaxuW4tlSAACYh3DjQwxLAQBgPsKNDzGhGAAA8xFufMhGzw0AAKYj3PhQ+5wbwg0AAOYh3PiQe84Nw1IAAJiGcONDTCgGAMB8hBsfYkIxAADmI9z4UBg9NwAAmI5w40M2NvEDAMB0hBsfYs4NAADmI9z4EOEGAADzEW58KIwJxQAAmI5w40P03AAAYD7CjQ+xFBwAAPMRbnyIZ0sBAGA+wo0Ptc+5cRpSK703AACYgnDjQ+1zbiSpxWGYWAkAAH0X4caHTg83DE0BAGAOwo0PhVotslhcXzc5HOYWAwBAH0W48SGLxXJqrxt6bgAAMAXhxsdOPV+KOTcAAJiBcONjbOQHAIC5CDc+RrgBAMBchBsfaw83Ta1MKAYAwAyEGx9jl2IAAMxFuPGxiLAQSVIT4QYAAFMQbnysveemsYVhKQAAzEC48TFbKD03AACYiXDjYzYmFAMAYCrCjY8x5wYAAHMRbnyMOTcAAJiLcONjtrC2YakWem4AADAD4cbHmFAMAIC5CDc+1t5zw7AUAADmINz4GD03AACYi3DjYywFBwDAXIQbHzsVbui5AQDADIQbH2vf54Y5NwAAmINw42P03AAAYC7CjY/Z2ncoZp8bAABMQbjxsQgmFAMAYCrCjY/Z3HNu6LkBAMAMhBsfYyk4AADmItz4GBOKAQAwF+HGx9qXghNuAAAwB+HGx9p7btjnBgAAcxBufMxGzw0AAKYi3PhYe89Nc6tThmGYXA0AAH0P4cbH2ufcSPTeAABgBsKNj7X33EjsUgwAgBkINz4WarXIanF9zV43AAAEHuHGxywWi2yhTCoGAMAshBs/iAhjOTgAAGYh3PgBPTcAAJiHcOMHtjCeLwUAgFkIN34Q0d5zw2opAAACjnDjB+09N4303AAAEHCmh5unnnpK2dnZioiIUF5entavX+/1+uXLl2vEiBGKjIyU3W7XbbfdpsbGxgBV2zXuJ4PTcwMAQMCZGm5eeuklLVy4UPfee682b96s3NxczZo1S6WlpZ1e/7e//U2LFi3Svffeq127dunZZ5/VSy+9pF//+tcBrtw7JhQDAGAeU8PNsmXLdP3112vevHkaPXq0VqxYoaioKD333HOdXv/pp59qxowZuuqqq5Sdna0LLrhAV1555Vf29gRaBBOKAQAwjWnhprm5WZs2bdLMmTNPFWO1aubMmVq7dm2n90yfPl2bNm1yh5mDBw/q7bff1kUXXXTG92lqalJNTY3Hy9/ae24aGZYCACDgQs164/LycjkcDqWnp3scT09P1+7duzu956qrrlJ5ebnOPvtsGYah1tZW3XjjjV6HpZYuXar777/fp7V/FfecG3puAAAIONMnFHfHmjVr9NBDD+l3v/udNm/erNdff13/+te/9OCDD57xnsWLF6u6utr9Kiws9Hud7n1u6LkBACDgTOu5SUlJUUhIiEpKSjyOl5SUKCMjo9N77r77bl1zzTX6+c9/LknKyclRfX29brjhBt15552yWjtmNZvNJpvN5vsP4IV7WIqeGwAAAs60npvw8HBNmjRJ+fn57mNOp1P5+fmaNm1ap/c0NDR0CDAhIa4gYRiG/4rtJnpuAAAwj2k9N5K0cOFCzZ07V5MnT9bUqVO1fPly1dfXa968eZKkOXPmKCsrS0uXLpUkXXLJJVq2bJkmTJigvLw87d+/X3fffbcuueQSd8jpCSLouQEAwDSmhpvZs2errKxM99xzj4qLizV+/HitXLnSPcm4oKDAo6fmrrvuksVi0V133aVjx44pNTVVl1xyiZYsWWLWR+hUZLgr3JxspucGAIBAsxg9aTwnAGpqahQfH6/q6mrFxcX55T3+9Olh3fvPHbo4p5+eunqiX94DAIC+pDu/v3vVaqneIjLM1XPT0NxqciUAAPQ9hBs/cA9LtTDnBgCAQCPc+EF7z81JVksBABBwhBs/aO+5aWym5wYAgEAj3PhBe7hpaGHODQAAgUa48QP3sBRLwQEACDjCjR+0h5tGJhQDABBwhBs/iAo/tRS8j20jBACA6Qg3fhDRFm6chtTsYGgKAIBAItz4QfuwlCQ1Mu8GAICAItz4QViIVaFWiyRWTAEAEGiEGz859fBMJhUDABBIhBs/ObVLMeEGAIBAItz4iXuXYsINAAABRbjxk1NPBifcAAAQSIQbP2HODQAA5iDc+AlzbgAAMAfhxk9OPV+KcAMAQCARbvzEPSxFzw0AAAFFuPEThqUAADAH4cZPmFAMAIA5CDd+QrgBAMAchBs/YVgKAABzEG78hHADAIA5CDd+EsWwFAAApiDc+EkEPTcAAJiCcOMnTCgGAMAchBs/Yc4NAADmCP06NzmdTu3fv1+lpaVyOp0e58455xyfFNbb0XMDAIA5uh1uPvvsM1111VU6cuSIDMPwOGexWORw8MtcoucGAACzdDvc3HjjjZo8ebL+9a9/qV+/frJYLP6oq9ej5wYAAHN0O9zs27dPr776qoYOHeqPeoJGdLiraeubW02uBACAvqXbE4rz8vK0f/9+f9QSVKJtrnDT2OJUq8P5FVcDAABf6XbPzc0336z//M//VHFxsXJychQWFuZxfty4cT4rrjeLtoW4v65vdig+koVpAAAEQrfDzQ9+8ANJ0nXXXec+ZrFYZBgGE4pPYwsNUViIRS0OQ3VNrYqPDPvqmwAAwDfW7XBz6NAhf9QRlGJsoTrR0KL6JubdAAAQKN0ONwMHDvRHHUEpui3c1BFuAAAImK+1id+BAwe0fPly7dq1S5I0evRo3XrrrRoyZIhPi+vtYtomFdNzAwBA4HR7luuqVas0evRorV+/XuPGjdO4ceO0bt06jRkzRqtXr/ZHjb1WNOEGAICA63bPzaJFi3Tbbbfp4Ycf7nD8jjvu0He/+12fFdfbtYebuiYmWQMAECjd7rnZtWuXfvazn3U4ft1112nnzp0+KSpYxLQtB69rbDG5EgAA+o5uh5vU1FRt3bq1w/GtW7cqLS3NFzUFjVO7FNNzAwBAoHR7WOr666/XDTfcoIMHD2r69OmSpE8++US/+c1vtHDhQp8X2JvFRLQPSzHnBgCAQOl2uLn77rsVGxurRx99VIsXL5YkZWZm6r777tMtt9zi8wJ7M1ZLAQAQeN0ONxaLRbfddptuu+021dbWSpJiY2N9XlgwODWhmHADAECgfK19btoRarxjKTgAAIHXpXAzceJE5efnKzExURMmTJDFYjnjtZs3b/ZZcb1d+2qpepaCAwAQMF0KN5deeqlsNpv7a2/hBqe0r5aqpecGAICA6VK4uffee91f33ffff6qJegwoRgAgMDr9j43gwcPVkVFRYfjVVVVGjx4sE+KChbtS8EJNwAABE63w83hw4flcHScQ9LU1KSjR4/6pKhgwWopAAACr8urpf75z3+6v161apXi4+Pd3zscDuXn52vQoEG+ra6XO31YyjAM5ioBABAAXQ43l112mSTXPjdz5871OBcWFqbs7Gw9+uijPi2ut2vvuXEaUmOLU5HhISZXBABA8OtyuHE6nZKkQYMGacOGDUpJSfFbUcEiKuxUmKlraiXcAAAQAN2ec3Po0CGCTRdZrRZFtwUa5t0AABAY3Q43t9xyix5//PEOx5988kktWLDAFzUFFXYpBgAgsLodbl577TXNmDGjw/Hp06fr1Vdf9UlRwSSGFVMAAARUt8NNRUWFx0qpdnFxcSovL/dJUcEktm2vm7pGwg0AAIHQ7XAzdOhQrVy5ssPxd955h038OhEXGSZJqj7ZYnIlAAD0Dd1+KvjChQs1f/58lZWV6Tvf+Y4kKT8/X48++qiWL1/u6/p6PcINAACB1e1wc91116mpqUlLlizRgw8+KEnKzs7W008/rTlz5vi8wN4unnADAEBAdTvcSNIvfvEL/eIXv1BZWZkiIyMVExPj67qCBuEGAIDA+lrhpl1qaqqv6gha7eGmhnADAEBAdHtCcUlJia655hplZmYqNDRUISEhHi94oucGAIDA6nbPzbXXXquCggLdfffd6tevHw+D/AqEGwAAAqvb4ebjjz/WRx99pPHjx/uhnOBDuAEAILC6PSxlt9tlGIY/aglKhBsAAAKr2+Fm+fLlWrRokQ4fPuyHcoIP4QYAgMDqdriZPXu21qxZoyFDhig2NlZJSUker6/jqaeeUnZ2tiIiIpSXl6f169ef8drzzjtPFoulw+viiy/+Wu/tb+2b+DW1OtXY4jC5GgAAgl+359z4ehfil156SQsXLtSKFSuUl5en5cuXa9asWdqzZ4/S0tI6XP/666+rubnZ/X1FRYVyc3P1ox/9yKd1+UqsLVQWi2QYUk1jiyLCWFEGAIA/dTvczJ0716cFLFu2TNdff73mzZsnSVqxYoX+9a9/6bnnntOiRYs6XP/l3qEXX3xRUVFRPTbcWK0WxUWEqfpki2pOtigtNsLskgAACGrdDjcFBQVezw8YMKDLP6u5uVmbNm3S4sWL3cesVqtmzpyptWvXdulnPPvss/rJT36i6OjoTs83NTWpqanJ/X1NTU2X6/OV+EhXuGHeDQAA/tftcJOdne11bxuHo+vzSsrLy+VwOJSenu5xPD09Xbt37/7K+9evX6/t27fr2WefPeM1S5cu1f3339/lmvyBScUAAAROt8PNli1bPL5vaWnRli1btGzZMi1ZssRnhXXFs88+q5ycHE2dOvWM1yxevFgLFy50f19TUyO73R6I8twINwAABE63w01ubm6HY5MnT1ZmZqYeeeQRXXHFFV3+WSkpKQoJCVFJSYnH8ZKSEmVkZHi9t76+Xi+++KIeeOABr9fZbDbZbLYu1+QP7nDTQLgBAMDfur0U/ExGjBihDRs2dOue8PBwTZo0Sfn5+e5jTqdT+fn5mjZtmtd7X3nlFTU1NemnP/3p16o3kOIiXRmy+mSryZUAABD8ut1z8+UJuYZhqKioSPfdd5+GDRvW7QIWLlyouXPnavLkyZo6daqWL1+u+vp69+qpOXPmKCsrS0uXLvW479lnn9Vll12m5OTkbr9noMUxLAUAQMB0O9wkJCR0mFBsGIbsdrtefPHFbhcwe/ZslZWV6Z577lFxcbHGjx+vlStXuicZFxQUyGr17GDas2ePPv74Y/373//u9vuZISEyXJJUdbL5K64EAADfVLfDzfvvv+/xvdVqVWpqqoYOHarQ0G7/OEnS/PnzNX/+/E7PrVmzpsOxESNG9KrnWyVGuXpuTtQTbgAA8LcupZGJEycqPz9fiYmJ+uCDD/SrX/1KUVFR/q4taCTHuCY0VxJuAADwuy5NKN61a5fq6+slSffff7/7a3RNUrRrWKqCcAMAgN91qedm/Pjxmjdvns4++2wZhqFHHnlEMTExnV57zz33+LTAYJDcFm7ouQEAwP+6FG6ef/553XvvvXrrrbdksVj0zjvvdDq/xmKxEG46kRTjCjcNzQ6dbHYoMpyHZwIA4C9dCjcjRoxwr4SyWq3Kz8/v9Ind6FysLVRhIRa1OAxV1DepfzjzlQAA8Jdub+LndDoJNt1ksVjc824YmgIAwL98tkMxvEuKdq2YYlIxAAD+RbgJkJS2eTeVdYQbAAD8iXATIAxLAQAQGISbAGkPN+V1TSZXAgBAcOt2uCksLNTRo0fd369fv14LFizQ73//e58WFmzSYiMkSWW1hBsAAPyp2+Hmqquucj9fqri4WN/97ne1fv163XnnnXrggQd8XmCwSIt1TSguJdwAAOBX3Q4327dv19SpUyVJL7/8ssaOHatPP/1UL7zwgp5//nlf1xc00uJc4aakptHkSgAACG7dDjctLS2y2Vy/qN999119//vflySNHDlSRUVFvq0uiKTHuYal6LkBAMC/uh1uxowZoxUrVuijjz7S6tWr9b3vfU+SdPz4cSUnJ/u8wGDRPixVfbJFjS0Ok6sBACB4dTvc/OY3v9Ezzzyj8847T1deeaVyc3MlSf/85z/dw1XoKD4yTOGhruZmUjEAAP7TpWdLne68885TeXm5ampqlJiY6D5+ww03KCqKZyadicViUVqsTUdPnFRpbaPsSbQVAAD+0O2em5MnT6qpqckdbI4cOaLly5drz549PHPqK7jn3dTQcwMAgL90O9xceuml+vOf/yxJqqqqUl5enh599FFddtllevrpp31eYDBpn3dTzIopAAD8ptvhZvPmzfrWt74lSXr11VeVnp6uI0eO6M9//rMef/xxnxcYTPrFR0qSiqoJNwAA+Eu3w01DQ4NiY2MlSf/+9791xRVXyGq16qyzztKRI0d8XmAwyUp0hZtjVSdNrgQAgODV7XAzdOhQvfHGGyosLNSqVat0wQUXSJJKS0sVFxfn8wKDSVaCa87NsROEGwAA/KXb4eaee+7Rr371K2VnZ2vq1KmaNm2aJFcvzoQJE3xeYDDJSnCtkDpOzw0AAH7T7aXgP/zhD3X22WerqKjIvceNJJ1//vm6/PLLfVpcsMlMOLVLcVOrQ7bQEJMrAgAg+HQ73EhSRkaGMjIy3E8H79+/Pxv4dUFSdLgiwqxqbHGqqKpR2SnRZpcEAEDQ6fawlNPp1AMPPKD4+HgNHDhQAwcOVEJCgh588EE5nU5/1Bg0LBaLMhNck4oZmgIAwD+63XNz55136tlnn9XDDz+sGTNmSJI+/vhj3XfffWpsbNSSJUt8XmQw6Z8YpYNl9So80WB2KQAABKVuh5s//elP+t///V/308Alady4ccrKytIvf/lLws1XyE6O0oeSDpUTbgAA8IduD0tVVlZq5MiRHY6PHDlSlZWVPikqmGUnu+bZHC6vN7kSAACCU7fDTW5urp588skOx5988kmP1VPo3KC2ScSHKwg3AAD4Q7eHpX7729/q4osv1rvvvuve42bt2rUqLCzU22+/7fMCg83AZNdeN0cqGmQYhiwWi8kVAQAQXLrdc3Puuedq7969uvzyy1VVVaWqqipdccUV2rNnj/uZUzgze1KUQqwWnWxxqISngwMA4HNfa5+bzMzMDhOHjx49qhtuuEG///3vfVJYsAoLsap/YqSOVDToYHmdMuIjzC4JAICg0u2emzOpqKjQs88+66sfF9SGpbkePLqvpM7kSgAACD4+CzfouhEZMZKkPSW1JlcCAEDwIdyYYHi6q+dmbzHhBgAAXyPcmGBEhivc7CmplWEYJlcDAEBw6fKE4iuuuMLr+aqqqm9aS58xOCVGoVaLahtbdby6UVltz5sCAADfXJfDTXx8/FeenzNnzjcuqC8ID7VqWHqsdhXV6Iuj1YQbAAB8qMvh5o9//KM/6+hzcvvHa1dRjbYdrdL3xmaYXQ4AAEGDOTcmybUnSJI+P1plah0AAAQbwo1JxvV3DfN9frRaTieTigEA8BXCjUmGp8cqKjxEtY2t2lvKknAAAHyFcGOSsBCrJg1MlCR9dqDC5GoAAAgehBsTnTU4WZL02cFKkysBACB4EG5M5A43hyrkYN4NAAA+QbgxUW7/eCVEhamqoUWbjpwwuxwAAIIC4cZEoSFWfWdkmiTp3zuKTa4GAIDgQLgx2QWjXRv4/XtnCc+ZAgDABwg3JjtneIpsoVYVVDZoVxFLwgEA+KYINyaLCg/Vt0e4hqZe2VRocjUAAPR+hJse4Mq8AZKk1zYd1clmh8nVAADQuxFueoBvDU2RPSlSNY2t+r9tx80uBwCAXo1w0wNYrRZdnTdQkvTMhwfY8wYAgG+AcNNDXJ03QAlRYTpQVq9/bDlmdjkAAPRahJseIjYiTL88b4gk6f9fvVcNza0mVwQAQO9EuOlB5kzLVlZCpI5VndRvV+4xuxwAAHolwk0PEhEWoqVX5EiSnv/0sD7cW2ZyRQAA9D6Emx7mnOGpuqptafj8v23WwbI6kysCAKB3Idz0QPdeMloTBySoprFV1zy7XoWVDWaXBABAr0G46YFsoSF65prJGpwSrWNVJ/XjZ9Zq+7Fqs8sCAKBXINz0UKmxNr14w1kamhajoupG/XDFp3p981EergkAwFcg3PRgaXEReu0X03Xu8FQ1tji18OVtuuEvm1Ra02h2aQAA9FiEmx4uPjJMz107RQu/O1xhIRat3lmi85d9oKfXHFBjC8+hAgDgyyxGHxvnqKmpUXx8vKqrqxUXF2d2Od2yu7hGt7/6uT4/6pp/0y8+QrecP0xXTMySLTTE5OoAAPCf7vz+Jtz0Mk6noTe2HtP/rNqj49Wu4am0WJt+/q1BuipvoGJsoSZXCACA7xFuvOjt4aZdY4tDf/3siP73o0MqbpuDExcRqh9PtuunZw1Udkq0yRUCAOA7hBsvgiXctGtudeqNrcf0zAcHdKCs3n383OGpmjNtoM4bkaYQq8XECgEA+Oa68/vb9AnFTz31lLKzsxUREaG8vDytX7/e6/VVVVW66aab1K9fP9lsNg0fPlxvv/12gKrtecJDrfrxZLtW33au/njtFH17RKosFumDvWX62Z826txH3tfv1uxnhRUAoM8wtefmpZde0pw5c7RixQrl5eVp+fLleuWVV7Rnzx6lpaV1uL65uVkzZsxQWlqafv3rXysrK0tHjhxRQkKCcnNzu/SewdZz05kjFfV6YV2BXt5YqKqGFklSiNWib49I0+wpdn17RKpCQ0zPtQAAdFmvGZbKy8vTlClT9OSTT0qSnE6n7Ha7br75Zi1atKjD9StWrNAjjzyi3bt3Kyws7Gu9Z18IN+0aWxz6v23H9dKGQm08csJ9PDXWph9O6q8fT7ZrEHNzAAC9QK8IN83NzYqKitKrr76qyy67zH187ty5qqqq0ptvvtnhnosuukhJSUmKiorSm2++qdTUVF111VW64447FBLS+VLopqYmNTU1ub+vqamR3W7vE+HmdPtL6/TyxkK9tumoKuqb3cenDkrST6bYdeHYfooMZzk5AKBn6hVzbsrLy+VwOJSenu5xPD09XcXFxZ3ec/DgQb366qtyOBx6++23dffdd+vRRx/Vf//3f5/xfZYuXar4+Hj3y263+/Rz9BZD02L064tGae3i87XipxP17RGpslqk9YcqtfDlbZqy5F3d/uo2rTtYIaezT80xBwAEGdN6bo4fP66srCx9+umnmjZtmvv47bffrg8++EDr1q3rcM/w4cPV2NioQ4cOuXtqli1bpkceeURFRUWdvg89N2dWVH1Sr248qpc3Faqw8qT7uD0pUpdP6K8fTMzSwGSGrQAA5utOz41pO76lpKQoJCREJSUlHsdLSkqUkZHR6T39+vVTWFiYxxDUqFGjVFxcrObmZoWHh3e4x2azyWaz+bb4INEvPlI3nz9MN317qDYcrtRrm4/q7S+KVVh5Uo/n79Pj+fs0eWCifjCpvy7K6af4yK83zwkAgEAybVgqPDxckyZNUn5+vvuY0+lUfn6+R0/O6WbMmKH9+/fL6XS6j+3du1f9+vXrNNiga6xWi/IGJ+u3P8zVhjtn6rGfjNe3hqXIapE2Hjmhxa9/oSlL3tX8v23W+3tK1epwfvUPBQDAJKYvBZ87d66eeeYZTZ06VcuXL9fLL7+s3bt3Kz09XXPmzFFWVpaWLl0qSSosLNSYMWM0d+5c3Xzzzdq3b5+uu+463XLLLbrzzju79J59abXUN1Vc3ag3th7Ta5uOal9pnft4aqxNl43P1GUTsjS6X5wsFjYJBAD4V68YlpKk2bNnq6ysTPfcc4+Ki4s1fvx4rVy50j3JuKCgQFbrqc4lu92uVatW6bbbbtO4ceOUlZWlW2+9VXfccYdZHyGoZcRH6MZzh+j/O2ewth+r0Wubj+rNrcdUVtukP3x0SH/46JCGpsXo0txMfX98JvNzAAA9Ao9fQLc0tzq1Zk+p/rHlmPJ3l6q59dQQ1Xh7gi4bn6mLx2UqNZZ5TgAA3+kV+9yYhXDjOzWNLVq1vVhvbj2uTw+Uq30FudUizRiaosvGZ+mCMemKjWAiMgDgmyHceEG48Y/Smka99XmR3tx2XNsKq9zHbaFWzRyVru+Pz9R5I1JlC2WjQABA9xFuvCDc+N+h8nr9c+txvbn1mA6Wn3pSeVxEqC4c20+X5GbqrMFJPN8KANBlhBsvCDeBYxiGdhyv0Ztbj+mf246rpObUZopJ0eH63tgM/UdOP00dRNABAHhHuPGCcGMOh9PQukMV+r9tx7Vye7FOtD2tXJJSYlxB5+KcTE0dlKQQK0vLAQCeCDdeEG7M1+Jw6rODFfrX50VauaNYVR5Bx6aLcjJ0cU4/Tc4m6AAAXAg3XhBuepYWh1OfHqjQvz4/rlU7SlR98lTQSYu16aKcfrp4XD9NGpAoK0EHAPoswo0XhJueq7nVqU8OlOtfnxdp1Y5i1Ta2us+lx9l04dh++t7YDE2hRwcA+hzCjReEm96hqdWhT/aX663Pi7R6R4lqm04FneTocH13dLpmjc3Q9CHJLC8HgD6AcOMF4ab3aWp16KO95Vq5o1ird3oOXcXYQvWdkWn63tgMnTs8VdE2U58oAgDwE8KNF4Sb3q3F4dT6Q5Vaub1Yq3YUq7T21PJyW6hV5wxP1ffGZOj8UWlKiOJJ8QAQLAg3XhBugofTaWjr0Sqt2l6sd7YXq6CywX0u1GrRWYOTNWtshmaNTldaXISJlQIAvinCjReEm+BkGIZ2F9e6e3R2F9e6z1ksrod6zhyVru+OTtewtBhZLExIBoDehHDjBeGmbzhUXq9VO1xBZ0tBlce5AUlRmjkqXTNHp2lKdpLC2B0ZAHo8wo0XhJu+p7i6Ufm7S/TuzhJ9cqBCza1O97m4iFB9e2Sazh+VrnOHpyo+kieYA0BPRLjxgnDTt9U3teqjfeV6d1eJ3ttdqsr6Zve5UKtFeYOTXL06o9JlT4oysVIAwOkIN14QbtDO4TS0peCEVu8qUf6uUu0vrfM4PzIjVjNHpev8UWnK7Z/ADskAYCLCjReEG5zJofJ65e8q0eqdJdpwuFLO0/5mJEeH69zhqTpvZJrOGZbCMnMACDDCjReEG3TFifpmrdlbqtU7S/TR3nKPHZKtFmnigER9e2Savj0iTaP6xbL6CgD8jHDjBeEG3dXicGrj4RNas6dU7+8p1d4Sz+Gr9Dibvj0iTeeNSNPZw1IUwy7JAOBzhBsvCDf4po6eaNCaPWVas6dUn+yv0MkWh/tcWIhFU7KT9O0Rafr2yFQNSWVPHQDwBcKNF4Qb+FJji0PrD1Xq/T2lWrOnTIfK6z3O90+M1HkjUnXOsFRNG5Ks2AiWmgPA10G48YJwA386VF7fNnxVps8Oeu6pE2q1aOKARH1rWIrOGZ6qsVnxCmEFFgB0CeHGC8INAqWhuVWf7q/Qh/vK9NG+8g69OglRYZoxNEXnDkvVt4anqF98pEmVAkDPR7jxgnADsxRWNriCzt5yfXKgXLWNrR7nh6bF6Jy2oHPWoGRFhoeYVCkA9DyEGy8IN+gJWh1ObTtapQ/2luujfWXaVljlsa9OeIhVUwYl6lvDXPN1RmbEsokggD6NcOMF4QY9UXVDiz454Ao6H+4t17Gqkx7nk6LDNW1IsmYMSdGMockakBTFKiwAfQrhxgvCDXo6wzB0sLxeH+0t04f7yvXZwQo1NDs8rslKiNT0IcmaMTRF04cmKy02wqRqASAwCDdeEG7Q2zS3OvX50Sp9sr9Cnxwo15aCE2pxeP61HZYW4wo6Q5J11pBkxbHkHECQIdx4QbhBb9fQ3KoNh0/o0/2uick7jtfo9L/FVouU0z9BM9p6diYNTFREGJOTAfRuhBsvCDcINlUNzVp7wNWr8+mBCh0s81xyHh5q1aQBicobnKSzBidrvD2BsAOg1yHceEG4QbArqj6pT9uGsD7dX6HimkaP8+GhVk2wJyhvcLLOGpykiQPo2QHQ8xFuvCDcoC9pn5z82cEKrTtYqc8OVqi0tsnjmvAQq8bbE9w9OxMHJLLHDoAeh3DjBeEGfZlhGDpc0aDPDla4XyU1nmEnLMSi3P6nws6kgYmKCudJ5wDMRbjxgnADnGIYho60hZ11h1w9O0XVnsNYoVaLcvrH66zByZqanaSJAxMVH8lqLACBRbjxgnADnJlhGCqsPOnq1TnkGsr68oaCFos0Ij1WU7KTNGVQkqZkJ/JcLAB+R7jxgnADdE9h5amenY2HK3W4oqHDNVkJkZqSndgWdpI0NDWGx0UA8CnCjReEG+CbKa1t1MbDJ7ThcKU2Hj6hHcerPZ6LJbmeeD55YKImZ7t6dsZmxcsWyiRlAF8f4cYLwg3gW3VNrdpScEIbDp/QxsOV2lJQpZMtno+LsIValWtPcPXuZCdpwgDm7QDoHsKNF4QbwL9aHE7tOF6jjYcrtf5QpTYeOaHK+uYO1w1Li9HEAYmaODBBEwckaghDWQC8INx4QbgBAqt9rx1X2DmhjUcqdaSTeTtxEaEaPyBRk9oCz3h7gmJ5RhaANoQbLwg3gPnK65q0paBKmwtOaPORE/r8aHWHoSyLRRqeFquJAxM0YUBiW+9OtCwWeneAvohw4wXhBuh5WhxO7S6qdYWdtldh5ckO1yVEhWmCPaFtOCtRufYExdjYYBDoCwg3XhBugN6htLaxQ+9OU6vT4xqrRRqWFqvx9gTl2hOUa4/XiPRYhYZYTaoagL8Qbrwg3AC9U3OrU7uKatp6dqq0+ciJDhsMSlJEmFVjM+Pbwk6CJtgT1D8xkuEsoJcj3HhBuAGCR2lNo7YWVmnb0SptK6zWtqNVqm1s7XBdUnS4cvufCjy5/ROUFB1uQsUAvi7CjReEGyB4OZ2GDlXUa2tBe+Cp0s6iGrU4Ov4zNyApyj2cNd4erzGZ8YoIY6NBoKci3HhBuAH6lqZWh3YV1WprwQltO1qtbYVVOlhe3+G6UKtFw9NjlZMVr7H94zUuK14jMmIJPEAPQbjxgnADoLqhRZ8fc/XsbC2s0tbCapXXNXW4rj3wjOsfr7FZ8crJitfIfrE8SgIwAeHGC8INgC8zDEPHqxv1xdEqfXGsWp8frdb2Y9U60dDS4dqwkI6BZ0QGgQfwN8KNF4QbAF1hGIaOVZ3U9raw88Ux74FnRIZrSCsnK0E5WfEanhFD4AF8iHDjBeEGwNdlGIaOnnAFni9Oe1V5CTxj+sVrdGacxmTGaVS/OEWz6SDwtRBuvCDcAPCl7gQei0XKTo7W6Mw4je7nCjxjMuOVGmszoXKgdyHceEG4AeBvpweenUU12nm8RjuO16i4prHT61NjbRrjDjzxGpMZpwFJUTwlHTgN4cYLwg0As1TUNWlnkSvouAJPtQ6W16uzf4VjbKEa1S/WHXhGZ8ZpWDrzeNB3EW68INwA6Ekamlu1u7jWHXh2Hq/W7uLaDs/RklxL04emxWh0vziN7BerkRmuP1NjbDxeAkGPcOMF4QZAT9fqcOpgeb12HK92D2ntOF6j6pMd5/FIUnJ0uDvsjMiI1agMVy8PGxAimBBuvCDcAOiN2vfi2Xm8RruLarS7pFa7i2p0qLxezk7+FbdapEEp0RrZL06jMk718mQl8BBR9E6EGy8INwCCSWOLQ/tK6rSruEa7i2q1u7hGu4pqOt2PR5JibaGevTz9YjUiI04xLFFHD0e48YJwAyDYGYahstom7Sp29e7sLq7V7uJa7S+t7fQhopJkT4rUiPRYDUuPbfszRkNSGdpCz0G48YJwA6CvanE4dbCsvq13x9XLs7uo9oxL1K1t+/IMT4/V8PQYV/DJiFV2crTCQ60Brh59HeHGC8INAHg6Ud+s3cW12ldaqz3FtdpXUqc9JbVnnMAcarVocGq0u5enPfgMTIpSaAihB/5BuPGCcAMAX619aGtvW9DZV1Lb9med6ppaO70nPNSqIakxGtEWdoa3hZ/+iZFsSIhvjHDjBeEGAL4+wzBUVN14KvAU12lfqSv0nGxxdHpPZFiIhqRFa2hqjIamnXoNTI5WGD096CLCjReEGwDwPafT9ciJve4enlrtKanTgbI6NXeyIaHkGt4amBzlEXiGpsZqSFq0osJZvQVPhBsvCDcAEDitDqeOVDZof2md9pfW6UBpnfaXuf6sb+68p0eSshIiNSQtpkNvT1J0eACrR09CuPGCcAMA5msf3moPPfvLToWfivrmM96XFB2uoakxruBz2qtfXATzeoIc4cYLwg0A9Gwn6pvdYcfd41NWp6MnTp7xnsiwEGWnRGtwarSGpERrUGq0BqfEaHBqtGIjwgJYPfyFcOMF4QYAeqeTzQ4dKHMFndODz+GK+jNuTihJqbE2DUqJ1pC2wDOoLQTZk6KY0NyL9Lpw89RTT+mRRx5RcXGxcnNz9cQTT2jq1KmdXvv8889r3rx5HsdsNpsaGzvfhOrLCDcAEFxaHU4VnjipQ+V1OlhWrwNl9TpYVqdD5fUqrW06432hVosGJEdpcEq0BqfGaHBKdFvwiVFKTDjP4OphuvP72/Tp6C+99JIWLlyoFStWKC8vT8uXL9esWbO0Z88epaWldXpPXFyc9uzZ4/6e/wEBoO8KDbFqUFsw+c5Iz3O1jS06VF6vg2X1OljuCj0Hy+p1qLxeJ1scruNl9dKuUo/7YiNCPUNPquvnZydHK5rncPV4pvfc5OXlacqUKXryySclSU6nU3a7XTfffLMWLVrU4frnn39eCxYsUFVVVZd+flNTk5qaTiX3mpoa2e12em4AoA9zOg0V1zS2BZ86V29Peb0Olbvm9nj7zZgaa9Og5GgNTI5SdlvgyU6J0sDkaB5A6ke9puemublZmzZt0uLFi93HrFarZs6cqbVr157xvrq6Og0cOFBOp1MTJ07UQw89pDFjxnR67dKlS3X//ff7vHYAQO9ltVqUmRCpzIRIzRia4nGuscWhIxUNrl4ed69PnQ6X1+tEQ4vKaptUVtuk9YcrO/zc1FibspOj2gKPK/i0hyCCT+CY2tLl5eVyOBxKT0/3OJ6enq7du3d3es+IESP03HPPady4caqurtb//M//aPr06dqxY4f69+/f4frFixdr4cKF7u/be24AAOhMRFiIRmS4HhL6ZdUNLTpcUe96lTfoSEW9DlXU60hFgyrrm93BZ8PhEx3uTYmxaVBbD8+glLbQ0xaCCD6+1etac9q0aZo2bZr7++nTp2vUqFF65pln9OCDD3a43mazyWazBbJEAECQio8KU25UgnLtCR3OVZ9scYWdclfYOVzeFoLagk95XZPK684cfLLdw1xRGpAcrYFJURqQFKWEqDDmlnaTqeEmJSVFISEhKikp8TheUlKijIyMLv2MsLAwTZgwQfv37/dHiQAAdEl8ZJjG9U/QuP4JHc5Vn2xRQUWDq5en/FRvz+HyelWcFnw2HukYfGJtoRqQ7Ao67X8OTIrWgKQoZSZE8CT2TpgabsLDwzVp0iTl5+frsssuk+SaUJyfn6/58+d36Wc4HA598cUXuuiii/xYKQAAX198ZJhy+scrp398h3M1jS06Ut7QNtTl6ukprGzQkcp6ldQ0qbapVTuO12jH8ZoO94ZYLcpKiPxS8ImSPSlKA5Oj+uwGhqYPSy1cuFBz587V5MmTNXXqVC1fvlz19fXuvWzmzJmjrKwsLV26VJL0wAMP6KyzztLQoUNVVVWlRx55REeOHNHPf/5zMz8GAABfS1zEmYNPY4tDhZUNKmh7HXEHH9efTa1O9zl1MoCRGBXWFnyiNSApUgOTot3BJyOIH1lheriZPXu2ysrKdM8996i4uFjjx4/XypUr3ZOMCwoKZLWe6nI7ceKErr/+ehUXFysxMVGTJk3Sp59+qtGjR5v1EQAA8IuIsBANS4/VsPSOk5udTkNldU06UtEWfirqXQGoLfiU1zXrREOLTjRUa9vR6g73h4dY1T8xUvakKNmTItU/MUr2RNfX9sTePdfH9H1uAo0digEAfUF9U6u7V6egLQC1B5+jJxq8PrJCkqLDQ2RPilL/xKhTISixLQQlRQZ8yKvXPX4hkAg3AIC+zuE0VFR9UgUVDSo80aCjJ06qsLJBhW1/entsRbuEqDBX6EmMagtBke6en6yEKEWGh/i05l6ziR8AAAi8EKulrUcmqtPzjS0OHas62dbLc9IVgCpP6ugJVwCqrG9WVUOLqhpatP1Yx4nOQ9Ni9O7Cc/39Mc6IcAMAADxEhIVoSGqMhqTGdHq+rqlVR9sCT+GJBhWeFnyOVjbInhgZ4Io9EW4AAEC3xNhCNTIjTiMzOg4PGYahplanCVWdws4/AADAZywWiyLCfDvfprsINwAAIKgQbgAAQFAh3AAAgKBCuAEAAEGFcAMAAIIK4QYAAAQVwg0AAAgqhBsAABBUCDcAACCoEG4AAEBQIdwAAICgQrgBAABBhXADAACCSqjZBQSaYRiSpJqaGpMrAQAAXdX+e7v997g3fS7c1NbWSpLsdrvJlQAAgO6qra1VfHy812ssRlciUBBxOp06fvy4YmNjZbFYfPqza2pqZLfbVVhYqLi4OJ/+bJxCOwcG7Rw4tHVg0M6B4a92NgxDtbW1yszMlNXqfVZNn+u5sVqt6t+/v1/fIy4ujr84AUA7BwbtHDi0dWDQzoHhj3b+qh6bdkwoBgAAQYVwAwAAggrhxodsNpvuvfde2Ww2s0sJarRzYNDOgUNbBwbtHBg9oZ373IRiAAAQ3Oi5AQAAQYVwAwAAggrhBgAABBXCDQAACCqEGx956qmnlJ2drYiICOXl5Wn9+vVml9SrLF26VFOmTFFsbKzS0tJ02WWXac+ePR7XNDY26qabblJycrJiYmL0gx/8QCUlJR7XFBQU6OKLL1ZUVJTS0tL0X//1X2ptbQ3kR+lVHn74YVksFi1YsMB9jHb2jWPHjumnP/2pkpOTFRkZqZycHG3cuNF93jAM3XPPPerXr58iIyM1c+ZM7du3z+NnVFZW6uqrr1ZcXJwSEhL0s5/9THV1dYH+KD2aw+HQ3XffrUGDBikyMlJDhgzRgw8+6PH8Idq6+z788ENdcsklyszMlMVi0RtvvOFx3ldt+vnnn+tb3/qWIiIiZLfb9dvf/tY3H8DAN/biiy8a4eHhxnPPPWfs2LHDuP76642EhASjpKTE7NJ6jVmzZhl//OMfje3btxtbt241LrroImPAgAFGXV2d+5obb7zRsNvtRn5+vrFx40bjrLPOMqZPn+4+39raaowdO9aYOXOmsWXLFuPtt982UlJSjMWLF5vxkXq89evXG9nZ2ca4ceOMW2+91X2cdv7mKisrjYEDBxrXXnutsW7dOuPgwYPGqlWrjP3797uvefjhh434+HjjjTfeMLZt22Z8//vfNwYNGmScPHnSfc33vvc9Izc31/jss8+Mjz76yBg6dKhx5ZVXmvGReqwlS5YYycnJxltvvWUcOnTIeOWVV4yYmBjjsccec19DW3ff22+/bdx5553G66+/bkgy/vGPf3ic90WbVldXG+np6cbVV19tbN++3fj73/9uREZGGs8888w3rp9w4wNTp041brrpJvf3DofDyMzMNJYuXWpiVb1baWmpIcn44IMPDMMwjKqqKiMsLMx45ZVX3Nfs2rXLkGSsXbvWMAzXX0ar1WoUFxe7r3n66aeNuLg4o6mpKbAfoIerra01hg0bZqxevdo499xz3eGGdvaNO+64wzj77LPPeN7pdBoZGRnGI4884j5WVVVl2Gw24+9//7thGIaxc+dOQ5KxYcMG9zXvvPOOYbFYjGPHjvmv+F7m4osvNq677jqPY1dccYVx9dVXG4ZBW/vCl8ONr9r0d7/7nZGYmOjx78Ydd9xhjBgx4hvXzLDUN9Tc3KxNmzZp5syZ7mNWq1UzZ87U2rVrTaysd6uurpYkJSUlSZI2bdqklpYWj3YeOXKkBgwY4G7ntWvXKicnR+np6e5rZs2apZqaGu3YsSOA1fd8N910ky6++GKP9pRoZ1/55z//qcmTJ+tHP/qR0tLSNGHCBP3hD39wnz906JCKi4s92jk+Pl55eXke7ZyQkKDJkye7r5k5c6asVqvWrVsXuA/Tw02fPl35+fnau3evJGnbtm36+OOPdeGFF0qirf3BV226du1anXPOOQoPD3dfM2vWLO3Zs0cnTpz4RjX2uQdn+lp5ebkcDofHP/SSlJ6ert27d5tUVe/mdDq1YMECzZgxQ2PHjpUkFRcXKzw8XAkJCR7Xpqenq7i42H1NZ/8d2s/B5cUXX9TmzZu1YcOGDudoZ984ePCgnn76aS1cuFC//vWvtWHDBt1yyy0KDw/X3Llz3e3UWTue3s5paWke50NDQ5WUlEQ7n2bRokWqqanRyJEjFRISIofDoSVLlujqq6+WJNraD3zVpsXFxRo0aFCHn9F+LjEx8WvXSLhBj3PTTTdp+/bt+vjjj80uJegUFhbq1ltv1erVqxUREWF2OUHL6XRq8uTJeuihhyRJEyZM0Pbt27VixQrNnTvX5OqCy8svv6wXXnhBf/vb3zRmzBht3bpVCxYsUGZmJm3dhzEs9Q2lpKQoJCSkw2qSkpISZWRkmFRV7zV//ny99dZbev/999W/f3/38YyMDDU3N6uqqsrj+tPbOSMjo9P/Du3n4Bp2Ki0t1cSJExUaGqrQ0FB98MEHevzxxxUaGqr09HTa2Qf69eun0aNHexwbNWqUCgoKJJ1qJ2//bmRkZKi0tNTjfGtrqyorK2nn0/zXf/2XFi1apJ/85CfKycnRNddco9tuu01Lly6VRFv7g6/a1J//lhBuvqHw8HBNmjRJ+fn57mNOp1P5+fmaNm2aiZX1LoZhaP78+frHP/6h9957r0NX5aRJkxQWFubRznv27FFBQYG7nadNm6YvvvjC4y/U6tWrFRcX1+EXTV91/vnn64svvtDWrVvdr8mTJ+vqq692f007f3MzZszosJXB3r17NXDgQEnSoEGDlJGR4dHONTU1WrdunUc7V1VVadOmTe5r3nvvPTmdTuXl5QXgU/QODQ0Nslo9f5WFhITI6XRKoq39wVdtOm3aNH344YdqaWlxX7N69WqNGDHiGw1JSWIpuC+8+OKLhs1mM55//nlj586dxg033GAkJCR4rCaBd7/4xS+M+Ph4Y82aNUZRUZH71dDQ4L7mxhtvNAYMGGC89957xsaNG41p06YZ06ZNc59vX6J8wQUXGFu3bjVWrlxppKamskT5K5y+WsowaGdfWL9+vREaGmosWbLE2Ldvn/HCCy8YUVFRxl//+lf3NQ8//LCRkJBgvPnmm8bnn39uXHrppZ0upZ0wYYKxbt064+OPPzaGDRvWp5cnd2bu3LlGVlaWeyn466+/bqSkpBi33367+xrauvtqa2uNLVu2GFu2bDEkGcuWLTO2bNliHDlyxDAM37RpVVWVkZ6eblxzzTXG9u3bjRdffNGIiopiKXhP8sQTTxgDBgwwwsPDjalTpxqfffaZ2SX1KpI6ff3xj390X3Py5Enjl7/8pZGYmGhERUUZl19+uVFUVOTxcw4fPmxceOGFRmRkpJGSkmL853/+p9HS0hLgT9O7fDnc0M6+8X//93/G2LFjDZvNZowcOdL4/e9/73He6XQad999t5Genm7YbDbj/PPPN/bs2eNxTUVFhXHllVcaMTExRlxcnDFv3jyjtrY2kB+jx6upqTFuvfVWY8CAAUZERIQxePBg48477/RYXkxbd9/777/f6b/Jc+fONQzDd226bds24+yzzzZsNpuRlZVlPPzwwz6p32IYp23jCAAA0Msx5wYAAAQVwg0AAAgqhBsAABBUCDcAACCoEG4AAEBQIdwAAICgQrgBAABBhXADAACCCuEGQJ+QnZ2t5cuXm10GgAAg3ADwuWuvvVaXXXaZJOm8887TggULAvbezz//vBISEjoc37Bhg2644YaA1QHAPKFmFwAAXdHc3Kzw8PCvfX9qaqoPqwHQk9FzA8Bvrr32Wn3wwQd67LHHZLFYZLFYdPjwYUnS9u3bdeGFFyomJkbp6em65pprVF5e7r73vPPO0/z587VgwQKlpKRo1qxZkqRly5YpJydH0dHRstvt+uUvf6m6ujpJ0po1azRv3jxVV1e73+++++6T1HFYqqCgQJdeeqliYmIUFxenH//4xyopKXGfv++++zR+/Hj95S9/UXZ2tuLj4/WTn/xEtbW17mteffVV5eTkKDIyUsnJyZo5c6bq6+v91JoAuopwA8BvHnvsMU2bNk3XX3+9ioqKVFRUJLvdrqqqKn3nO9/RhAkTtHHjRq1cuVIlJSX68Y9/7HH/n/70J4WHh+uTTz7RihUrJElWq1WPP/64duzYoT/96U967733dPvtt0uSpk+fruXLlysuLs79fr/61a861OV0OnXppZeqsrJSH3zwgVavXq2DBw9q9uzZHtcdOHBAb7zxht566y299dZb+uCDD/Twww9LkoqKinTllVfquuuu065du7RmzRpdccUV4lnEgPkYlgLgN/Hx8QoPD1dUVJQyMjLcx5988klNmDBBDz30kPvYc889J7vdrr1792r48OGSpGHDhum3v/2tx888ff5Odna2/vu//1s33nijfve73yk8PFzx8fGyWCwe7/dl+fn5+uKLL3To0CHZ7XZJ0p///GeNGTNGGzZs0JQpUyS5QtDzzz+v2NhYSdI111yj/Px8LVmyREVFRWptbdUVV1yhgQMHSpJycnK+QWsB8BV6bgAE3LZt2/T+++8rJibG/Ro5cqQkV29Ju0mTJnW4991339X555+vrKwsxcbG6pprrlFFRYUaGhq6/P67du2S3W53BxtJGj16tBISErRr1y73sezsbHewkaR+/fqptLRUkpSbm6vzzz9fOTk5+tGPfqQ//OEPOnHiRNcbAYDfEG4ABFxdXZ0uueQSbd261eO1b98+nXPOOe7roqOjPe47fPiw/uM//kPjxo3Ta6+9pk2bNumpp56S5Jpw7GthYWEe31ssFjmdTklSSEiIVq9erXfeeUejR4/WE088oREjRujQoUM+rwNA9xBuAPhVeHi4HA6Hx7GJEydqx44dys7O1tChQz1eXw40p9u0aZOcTqceffRRnXXWWRo+fLiOHz/+le/3ZaNGjVJhYaEKCwvdx3bu3KmqqiqNHj26y5/NYrFoxowZuv/++7VlyxaFh4frH//4R5fvB+AfhBsAfpWdna1169bp8OHDKi8vl9Pp1E033aTKykpdeeWV2rBhgw4cOKBVq1Zp3rx5XoPJ0KFD1dLSoieeeEIHDx7UX/7yF/dE49Pfr66uTvn5+SovL+90uGrmzJnKycnR1Vdfrc2bN2v9+vWaM2eOzj33XE2ePLlLn2vdunV66KGHtHHjRhUUFOj1119XWVmZRo0a1b0GAuBzhBsAfvWrX/1KISEhGj16tFJTU1VQUKDMzEx98skncjgcuuCCC5STk6MFCxYoISFBVuuZ/1nKzc3VsmXL9Jvf/EZjx47VCy+8oKVLl3pcM336dN14442aPXu2UlNTO0xIllw9Lm+++aYSExN1zjnnaObMmRo8eLBeeumlLn+uuLg4ffjhh7rooos0fPhw3XXXXXr00Ud14YUXdr1xAPiFxWDdIgAACCL03AAAgKBCuAEAAEGFcAMAAIIK4QYAAAQVwg0AAAgqhBsAABBUCDcAACCoEG4AAEBQIdwAAICgQrgBAABBhXADAACCyv8DCQHOAFxQhtMAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# PLotting the loss values for every training iterations\n",
        "\n",
        "loss_plot = [loss_hist[i] for i in range(len(loss_hist))]\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss function\")\n",
        "plt.plot(loss_plot)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgKAU9RBEUlL"
      },
      "source": [
        "### 2.7 Experimenting with different values of the Hyperparemeters\n",
        "\n",
        "Previously, we randomly sampled the learning rate and the number of features to train the model. Now, you have to manually choose the number of features and the learning rate. Then, you have to train the model again on the manually choosen hyperparameters (number of features and learning rate). In the next cell, you have to manually choose the hyperparameters and write the code to train the model.\n",
        "\n",
        "After the model is trained, you have to compare the performance of the model with random chosen hyperparameters and the model **with** manually chosen hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xMcc3cuvIbD_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nManually choose the hyperparameters (learning rate and number of features) and train the model.\\nThen compare the performance with random chosen hyperparameters and manually chosen hyperparameters.\\n'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "\"\"\"\n",
        "Manually choose the hyperparameters (learning rate and number of features) and train the model.\n",
        "Then compare the performance with random chosen hyperparameters and manually chosen hyperparameters.\n",
        "\"\"\"\n",
        "\n",
        "### START CODE HERE ###\n",
        "\n",
        "### END CODE HERE ###"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
